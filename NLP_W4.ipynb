{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP W4",
      "provenance": [],
      "authorship_tag": "ABX9TyP30ZLa9+HmZXdUB3rZhXJW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sissilin1027/Deep_Learning-/blob/main/NLP_W4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfXEwvYaSMdE"
      },
      "source": [
        "### Part I"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn1y18uNPxew"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm-HNCu6QDUq",
        "outputId": "cd8d4194-4cf2-4cf0-96d1-8bed464e47fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data=\"In the town of Athy one Jeremy Lanigan \\n Battered away til he hadnt a pound. \\nHis father died and made him a man again \\n Left him a farm and ten acres of ground. \\nHe gave a grand party for friends and relations \\nWho didnt forget him when come to the wall, \\nAnd if youll but listen Ill make your eyes glisten \\nOf the rows and the ructions of Lanigans Ball. \\nMyself to be sure got free invitation, \\nFor all the nice girls and boys I might ask, \\nAnd just in a minute both friends and relations \\nWere dancing round merry as bees round a cask. \\nJudy ODaly, that nice little milliner, \\nShe tipped me a wink for to give her a call, \\nAnd I soon arrived with Peggy McGilligan \\nJust in time for Lanigans Ball. \\nThere were lashings of punch and wine for the ladies, \\nPotatoes and cakes; there was bacon and tea, \\nThere were the Nolans, Dolans, OGradys \\nCourting the girls and dancing away. \\nSongs they went round as plenty as water, \\nThe harp that once sounded in Taras old hall,\\nSweet Nelly Gray and The Rat Catchers Daughter,\\nAll singing together at Lanigans Ball. \\nThey were doing all kinds of nonsensical polkas \\nAll round the room in a whirligig. \\nJulia and I, we banished their nonsense \\nAnd tipped them the twist of a reel and a jig. \\nAch mavrone, how the girls got all mad at me \\nDanced til youd think the ceiling would fall. \\nFor I spent three weeks at Brooks Academy \\nLearning new steps for Lanigans Ball. \\nThree long weeks I spent up in Dublin, \\nThree long weeks to learn nothing at all,\\n Three long weeks I spent up in Dublin, \\nLearning new steps for Lanigans Ball. \\nShe stepped out and I stepped in again, \\nI stepped out and she stepped in again, \\nShe stepped out and I stepped in again, \\nLearning new steps for Lanigans Ball. \\nBoys were all merry and the girls they were hearty \\nAnd danced all around in couples and groups, \\nTil an accident happened, young Terrance McCarthy \\nPut his right leg through miss Finnertys hoops. \\nPoor creature fainted and cried Meelia murther, \\nCalled for her brothers and gathered them all. \\nCarmody swore that hed go no further \\nTil he had satisfaction at Lanigans Ball. \\nIn the midst of the row miss Kerrigan fainted, \\nHer cheeks at the same time as red as a rose. \\nSome of the lads declared she was painted, \\nShe took a small drop too much, I suppose. \\nHer sweetheart, Ned Morgan, so powerful and able, \\nWhen he saw his fair colleen stretched out by the wall, \\nTore the left leg from under the table \\nAnd smashed all the Chaneys at Lanigans Ball. \\nBoys, oh boys, twas then there were runctions. \\nMyself got a lick from big Phelim McHugh. \\nI soon replied to his introduction \\nAnd kicked up a terrible hullabaloo. \\nOld Casey, the piper, was near being strangled. \\nThey squeezed up his pipes, bellows, chanters and all. \\nThe girls, in their ribbons, they got all entangled \\nAnd that put an end to Lanigans Ball.\"\n",
        "\n",
        "corpus = data.lower().split('\\n')\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n",
            "263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5otGwnutQS_C"
      },
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-vKA4fxQ7V6",
        "outputId": "1d783011-2877-4ba9-f1f1-cc408c8ce0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "2\n",
            "66\n",
            "8\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzJtcMQDRIPF",
        "outputId": "57192266-b0e7-41e2-a7ff-52e3e84bddeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(xs[6])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  4  2 66  8 67 68 69]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy41K7jURLLr",
        "outputId": "5a014ce5-cc11-4b5b-c715-d768ba16f661",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# One-hot encoding\n",
        "print(ys[6])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D63iaXNfRMYK",
        "outputId": "56cfbf3f-d7bd-4be7-9b4c-f4aa8b5c8e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  0  0  0  4  2 66  8 67 68]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYMo3S1mRN7O",
        "outputId": "dd2590a1-ec55-4d8e-d09a-77f041b30db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'and': 1, 'the': 2, 'a': 3, 'in': 4, 'all': 5, 'i': 6, 'for': 7, 'of': 8, 'lanigans': 9, 'ball': 10, 'were': 11, 'at': 12, 'to': 13, 'she': 14, 'stepped': 15, 'his': 16, 'girls': 17, 'as': 18, 'they': 19, 'til': 20, 'he': 21, 'again': 22, 'got': 23, 'boys': 24, 'round': 25, 'that': 26, 'her': 27, 'there': 28, 'three': 29, 'weeks': 30, 'up': 31, 'out': 32, 'him': 33, 'was': 34, 'spent': 35, 'learning': 36, 'new': 37, 'steps': 38, 'long': 39, 'away': 40, 'left': 41, 'friends': 42, 'relations': 43, 'when': 44, 'wall': 45, 'myself': 46, 'nice': 47, 'just': 48, 'dancing': 49, 'merry': 50, 'tipped': 51, 'me': 52, 'soon': 53, 'time': 54, 'old': 55, 'their': 56, 'them': 57, 'danced': 58, 'dublin': 59, 'an': 60, 'put': 61, 'leg': 62, 'miss': 63, 'fainted': 64, 'from': 65, 'town': 66, 'athy': 67, 'one': 68, 'jeremy': 69, 'lanigan': 70, 'battered': 71, 'hadnt': 72, 'pound': 73, 'father': 74, 'died': 75, 'made': 76, 'man': 77, 'farm': 78, 'ten': 79, 'acres': 80, 'ground': 81, 'gave': 82, 'grand': 83, 'party': 84, 'who': 85, 'didnt': 86, 'forget': 87, 'come': 88, 'if': 89, 'youll': 90, 'but': 91, 'listen': 92, 'ill': 93, 'make': 94, 'your': 95, 'eyes': 96, 'glisten': 97, 'rows': 98, 'ructions': 99, 'be': 100, 'sure': 101, 'free': 102, 'invitation': 103, 'might': 104, 'ask': 105, 'minute': 106, 'both': 107, 'bees': 108, 'cask': 109, 'judy': 110, 'odaly': 111, 'little': 112, 'milliner': 113, 'wink': 114, 'give': 115, 'call': 116, 'arrived': 117, 'with': 118, 'peggy': 119, 'mcgilligan': 120, 'lashings': 121, 'punch': 122, 'wine': 123, 'ladies': 124, 'potatoes': 125, 'cakes': 126, 'bacon': 127, 'tea': 128, 'nolans': 129, 'dolans': 130, 'ogradys': 131, 'courting': 132, 'songs': 133, 'went': 134, 'plenty': 135, 'water': 136, 'harp': 137, 'once': 138, 'sounded': 139, 'taras': 140, 'hall': 141, 'sweet': 142, 'nelly': 143, 'gray': 144, 'rat': 145, 'catchers': 146, 'daughter': 147, 'singing': 148, 'together': 149, 'doing': 150, 'kinds': 151, 'nonsensical': 152, 'polkas': 153, 'room': 154, 'whirligig': 155, 'julia': 156, 'we': 157, 'banished': 158, 'nonsense': 159, 'twist': 160, 'reel': 161, 'jig': 162, 'ach': 163, 'mavrone': 164, 'how': 165, 'mad': 166, 'youd': 167, 'think': 168, 'ceiling': 169, 'would': 170, 'fall': 171, 'brooks': 172, 'academy': 173, 'learn': 174, 'nothing': 175, 'hearty': 176, 'around': 177, 'couples': 178, 'groups': 179, 'accident': 180, 'happened': 181, 'young': 182, 'terrance': 183, 'mccarthy': 184, 'right': 185, 'through': 186, 'finnertys': 187, 'hoops': 188, 'poor': 189, 'creature': 190, 'cried': 191, 'meelia': 192, 'murther': 193, 'called': 194, 'brothers': 195, 'gathered': 196, 'carmody': 197, 'swore': 198, 'hed': 199, 'go': 200, 'no': 201, 'further': 202, 'had': 203, 'satisfaction': 204, 'midst': 205, 'row': 206, 'kerrigan': 207, 'cheeks': 208, 'same': 209, 'red': 210, 'rose': 211, 'some': 212, 'lads': 213, 'declared': 214, 'painted': 215, 'took': 216, 'small': 217, 'drop': 218, 'too': 219, 'much': 220, 'suppose': 221, 'sweetheart': 222, 'ned': 223, 'morgan': 224, 'so': 225, 'powerful': 226, 'able': 227, 'saw': 228, 'fair': 229, 'colleen': 230, 'stretched': 231, 'by': 232, 'tore': 233, 'under': 234, 'table': 235, 'smashed': 236, 'chaneys': 237, 'oh': 238, 'twas': 239, 'then': 240, 'runctions': 241, 'lick': 242, 'big': 243, 'phelim': 244, 'mchugh': 245, 'replied': 246, 'introduction': 247, 'kicked': 248, 'terrible': 249, 'hullabaloo': 250, 'casey': 251, 'piper': 252, 'near': 253, 'being': 254, 'strangled': 255, 'squeezed': 256, 'pipes': 257, 'bellows': 258, 'chanters': 259, 'ribbons': 260, 'entangled': 261, 'end': 262}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REir1R_4ROyt",
        "outputId": "76bd5a35-1e55-43ac-a877-2a488faaf2e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "  model = Sequential()\n",
        "  model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "  model.add(Bidirectional(LSTM(20)))\n",
        "  model.add(Dense(total_words, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(xs, ys, epochs=500, verbose=1)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.5697 - accuracy: 0.0132\n",
            "Epoch 2/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.5459 - accuracy: 0.0596\n",
            "Epoch 3/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.4923 - accuracy: 0.0508\n",
            "Epoch 4/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 5.3314 - accuracy: 0.0508\n",
            "Epoch 5/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.1462 - accuracy: 0.0508\n",
            "Epoch 6/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.0695 - accuracy: 0.0508\n",
            "Epoch 7/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 5.0334 - accuracy: 0.0508\n",
            "Epoch 8/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.9996 - accuracy: 0.0508\n",
            "Epoch 9/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.9723 - accuracy: 0.0530\n",
            "Epoch 10/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.9392 - accuracy: 0.0618\n",
            "Epoch 11/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.9043 - accuracy: 0.0640\n",
            "Epoch 12/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.8674 - accuracy: 0.0618\n",
            "Epoch 13/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.8331 - accuracy: 0.0552\n",
            "Epoch 14/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.7881 - accuracy: 0.0552\n",
            "Epoch 15/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.7403 - accuracy: 0.0596\n",
            "Epoch 16/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 4.6941 - accuracy: 0.0640\n",
            "Epoch 17/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.6561 - accuracy: 0.0662\n",
            "Epoch 18/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.6174 - accuracy: 0.0618\n",
            "Epoch 19/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.5849 - accuracy: 0.0773\n",
            "Epoch 20/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.5519 - accuracy: 0.0751\n",
            "Epoch 21/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.5205 - accuracy: 0.0773\n",
            "Epoch 22/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.4877 - accuracy: 0.0883\n",
            "Epoch 23/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.4504 - accuracy: 0.0861\n",
            "Epoch 24/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.4132 - accuracy: 0.0971\n",
            "Epoch 25/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.3850 - accuracy: 0.0905\n",
            "Epoch 26/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.3559 - accuracy: 0.0927\n",
            "Epoch 27/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.3098 - accuracy: 0.0993\n",
            "Epoch 28/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.2786 - accuracy: 0.1038\n",
            "Epoch 29/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 4.2505 - accuracy: 0.1015\n",
            "Epoch 30/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.2145 - accuracy: 0.1038\n",
            "Epoch 31/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 4.1828 - accuracy: 0.1060\n",
            "Epoch 32/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.1386 - accuracy: 0.1170\n",
            "Epoch 33/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.0967 - accuracy: 0.1170\n",
            "Epoch 34/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.0623 - accuracy: 0.1302\n",
            "Epoch 35/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 4.0248 - accuracy: 0.1258\n",
            "Epoch 36/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.9884 - accuracy: 0.1280\n",
            "Epoch 37/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.9514 - accuracy: 0.1369\n",
            "Epoch 38/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.9074 - accuracy: 0.1567\n",
            "Epoch 39/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.8605 - accuracy: 0.1567\n",
            "Epoch 40/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.8224 - accuracy: 0.1634\n",
            "Epoch 41/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.7828 - accuracy: 0.1788\n",
            "Epoch 42/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.7392 - accuracy: 0.1744\n",
            "Epoch 43/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.6992 - accuracy: 0.1788\n",
            "Epoch 44/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.6725 - accuracy: 0.1854\n",
            "Epoch 45/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.6349 - accuracy: 0.2031\n",
            "Epoch 46/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.5912 - accuracy: 0.1943\n",
            "Epoch 47/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.5497 - accuracy: 0.2075\n",
            "Epoch 48/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.5107 - accuracy: 0.2296\n",
            "Epoch 49/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.4850 - accuracy: 0.2450\n",
            "Epoch 50/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4468 - accuracy: 0.2517\n",
            "Epoch 51/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.4030 - accuracy: 0.2737\n",
            "Epoch 52/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.3742 - accuracy: 0.2980\n",
            "Epoch 53/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.3338 - accuracy: 0.2804\n",
            "Epoch 54/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.2874 - accuracy: 0.3002\n",
            "Epoch 55/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.2477 - accuracy: 0.3267\n",
            "Epoch 56/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.2112 - accuracy: 0.3289\n",
            "Epoch 57/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.1782 - accuracy: 0.3510\n",
            "Epoch 58/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.1516 - accuracy: 0.3598\n",
            "Epoch 59/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.1250 - accuracy: 0.3598\n",
            "Epoch 60/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.1067 - accuracy: 0.3731\n",
            "Epoch 61/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 3.0565 - accuracy: 0.3775\n",
            "Epoch 62/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 3.0358 - accuracy: 0.3996\n",
            "Epoch 63/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 3.0096 - accuracy: 0.3907\n",
            "Epoch 64/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.9550 - accuracy: 0.4128\n",
            "Epoch 65/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.9166 - accuracy: 0.4172\n",
            "Epoch 66/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.8846 - accuracy: 0.4459\n",
            "Epoch 67/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.8579 - accuracy: 0.4481\n",
            "Epoch 68/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.8222 - accuracy: 0.4459\n",
            "Epoch 69/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.7798 - accuracy: 0.4636\n",
            "Epoch 70/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.7515 - accuracy: 0.4702\n",
            "Epoch 71/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.7239 - accuracy: 0.4768\n",
            "Epoch 72/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.6913 - accuracy: 0.5055\n",
            "Epoch 73/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.6506 - accuracy: 0.5232\n",
            "Epoch 74/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.6201 - accuracy: 0.5430\n",
            "Epoch 75/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.5939 - accuracy: 0.5541\n",
            "Epoch 76/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.5658 - accuracy: 0.5364\n",
            "Epoch 77/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.5366 - accuracy: 0.5519\n",
            "Epoch 78/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.5035 - accuracy: 0.5585\n",
            "Epoch 79/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4790 - accuracy: 0.5541\n",
            "Epoch 80/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4544 - accuracy: 0.5651\n",
            "Epoch 81/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.4252 - accuracy: 0.5806\n",
            "Epoch 82/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.3949 - accuracy: 0.5982\n",
            "Epoch 83/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 2.3709 - accuracy: 0.5784\n",
            "Epoch 84/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.3477 - accuracy: 0.5938\n",
            "Epoch 85/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.3141 - accuracy: 0.5784\n",
            "Epoch 86/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2873 - accuracy: 0.6004\n",
            "Epoch 87/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2556 - accuracy: 0.6026\n",
            "Epoch 88/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2266 - accuracy: 0.6159\n",
            "Epoch 89/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.2004 - accuracy: 0.6225\n",
            "Epoch 90/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1857 - accuracy: 0.6203\n",
            "Epoch 91/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 2.1659 - accuracy: 0.6313\n",
            "Epoch 92/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1319 - accuracy: 0.6358\n",
            "Epoch 93/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.1066 - accuracy: 0.6578\n",
            "Epoch 94/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.0870 - accuracy: 0.6534\n",
            "Epoch 95/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 2.0628 - accuracy: 0.6645\n",
            "Epoch 96/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.0418 - accuracy: 0.6556\n",
            "Epoch 97/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 2.0125 - accuracy: 0.6733\n",
            "Epoch 98/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9894 - accuracy: 0.6755\n",
            "Epoch 99/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9653 - accuracy: 0.6865\n",
            "Epoch 100/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9357 - accuracy: 0.6909\n",
            "Epoch 101/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.9194 - accuracy: 0.6976\n",
            "Epoch 102/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9198 - accuracy: 0.6865\n",
            "Epoch 103/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.9008 - accuracy: 0.6865\n",
            "Epoch 104/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.8887 - accuracy: 0.6954\n",
            "Epoch 105/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.8553 - accuracy: 0.6998\n",
            "Epoch 106/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.8299 - accuracy: 0.7086\n",
            "Epoch 107/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.7972 - accuracy: 0.7152\n",
            "Epoch 108/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7760 - accuracy: 0.7108\n",
            "Epoch 109/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.7500 - accuracy: 0.7263\n",
            "Epoch 110/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.7375 - accuracy: 0.7263\n",
            "Epoch 111/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.7218 - accuracy: 0.7196\n",
            "Epoch 112/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.7054 - accuracy: 0.7196\n",
            "Epoch 113/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6777 - accuracy: 0.7307\n",
            "Epoch 114/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6638 - accuracy: 0.7329\n",
            "Epoch 115/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.6576 - accuracy: 0.7241\n",
            "Epoch 116/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.6274 - accuracy: 0.7483\n",
            "Epoch 117/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.5959 - accuracy: 0.7550\n",
            "Epoch 118/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.5755 - accuracy: 0.7660\n",
            "Epoch 119/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.5591 - accuracy: 0.7638\n",
            "Epoch 120/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.5496 - accuracy: 0.7594\n",
            "Epoch 121/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.5262 - accuracy: 0.7638\n",
            "Epoch 122/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.5213 - accuracy: 0.7682\n",
            "Epoch 123/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.5059 - accuracy: 0.7792\n",
            "Epoch 124/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.4942 - accuracy: 0.7903\n",
            "Epoch 125/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4969 - accuracy: 0.7770\n",
            "Epoch 126/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.4682 - accuracy: 0.7837\n",
            "Epoch 127/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.4731 - accuracy: 0.7859\n",
            "Epoch 128/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.4730 - accuracy: 0.7748\n",
            "Epoch 129/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4200 - accuracy: 0.7969\n",
            "Epoch 130/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.4016 - accuracy: 0.8057\n",
            "Epoch 131/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.3768 - accuracy: 0.8124\n",
            "Epoch 132/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.3551 - accuracy: 0.8168\n",
            "Epoch 133/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.3384 - accuracy: 0.8190\n",
            "Epoch 134/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.3509 - accuracy: 0.8079\n",
            "Epoch 135/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.3180 - accuracy: 0.8146\n",
            "Epoch 136/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.3073 - accuracy: 0.8146\n",
            "Epoch 137/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2949 - accuracy: 0.8212\n",
            "Epoch 138/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.2713 - accuracy: 0.8322\n",
            "Epoch 139/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2604 - accuracy: 0.8300\n",
            "Epoch 140/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2450 - accuracy: 0.8322\n",
            "Epoch 141/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2249 - accuracy: 0.8366\n",
            "Epoch 142/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2239 - accuracy: 0.8322\n",
            "Epoch 143/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.2089 - accuracy: 0.8344\n",
            "Epoch 144/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.2037 - accuracy: 0.8344\n",
            "Epoch 145/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1876 - accuracy: 0.8366\n",
            "Epoch 146/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1668 - accuracy: 0.8389\n",
            "Epoch 147/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1513 - accuracy: 0.8477\n",
            "Epoch 148/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.1443 - accuracy: 0.8322\n",
            "Epoch 149/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1198 - accuracy: 0.8477\n",
            "Epoch 150/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.1065 - accuracy: 0.8543\n",
            "Epoch 151/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0987 - accuracy: 0.8521\n",
            "Epoch 152/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 1.0891 - accuracy: 0.8565\n",
            "Epoch 153/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 1.0758 - accuracy: 0.8609\n",
            "Epoch 154/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.0690 - accuracy: 0.8587\n",
            "Epoch 155/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0522 - accuracy: 0.8720\n",
            "Epoch 156/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0498 - accuracy: 0.8631\n",
            "Epoch 157/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 1.0377 - accuracy: 0.8675\n",
            "Epoch 158/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0230 - accuracy: 0.8720\n",
            "Epoch 159/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 1.0083 - accuracy: 0.8764\n",
            "Epoch 160/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9956 - accuracy: 0.8764\n",
            "Epoch 161/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.9838 - accuracy: 0.8786\n",
            "Epoch 162/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9730 - accuracy: 0.8720\n",
            "Epoch 163/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9610 - accuracy: 0.8720\n",
            "Epoch 164/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9510 - accuracy: 0.8764\n",
            "Epoch 165/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9394 - accuracy: 0.8720\n",
            "Epoch 166/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.9303 - accuracy: 0.8742\n",
            "Epoch 167/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9227 - accuracy: 0.8808\n",
            "Epoch 168/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9112 - accuracy: 0.8808\n",
            "Epoch 169/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.9016 - accuracy: 0.8830\n",
            "Epoch 170/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8930 - accuracy: 0.8830\n",
            "Epoch 171/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8826 - accuracy: 0.8874\n",
            "Epoch 172/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8722 - accuracy: 0.8896\n",
            "Epoch 173/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8654 - accuracy: 0.8830\n",
            "Epoch 174/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.8558 - accuracy: 0.8808\n",
            "Epoch 175/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.8486 - accuracy: 0.8874\n",
            "Epoch 176/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8391 - accuracy: 0.8830\n",
            "Epoch 177/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8295 - accuracy: 0.8918\n",
            "Epoch 178/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.8186 - accuracy: 0.8918\n",
            "Epoch 179/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8105 - accuracy: 0.8962\n",
            "Epoch 180/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.8065 - accuracy: 0.8985\n",
            "Epoch 181/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7961 - accuracy: 0.8962\n",
            "Epoch 182/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7904 - accuracy: 0.8918\n",
            "Epoch 183/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7839 - accuracy: 0.9007\n",
            "Epoch 184/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7760 - accuracy: 0.8985\n",
            "Epoch 185/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.7693 - accuracy: 0.9007\n",
            "Epoch 186/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7685 - accuracy: 0.8962\n",
            "Epoch 187/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7611 - accuracy: 0.8918\n",
            "Epoch 188/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7502 - accuracy: 0.9007\n",
            "Epoch 189/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7834 - accuracy: 0.8852\n",
            "Epoch 190/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7771 - accuracy: 0.8918\n",
            "Epoch 191/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7614 - accuracy: 0.9007\n",
            "Epoch 192/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7368 - accuracy: 0.9051\n",
            "Epoch 193/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7231 - accuracy: 0.9051\n",
            "Epoch 194/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.7497 - accuracy: 0.8896\n",
            "Epoch 195/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7579 - accuracy: 0.8985\n",
            "Epoch 196/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7413 - accuracy: 0.8896\n",
            "Epoch 197/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7153 - accuracy: 0.9007\n",
            "Epoch 198/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.7070 - accuracy: 0.9073\n",
            "Epoch 199/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6933 - accuracy: 0.9095\n",
            "Epoch 200/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6823 - accuracy: 0.9051\n",
            "Epoch 201/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6771 - accuracy: 0.9073\n",
            "Epoch 202/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6611 - accuracy: 0.9117\n",
            "Epoch 203/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6472 - accuracy: 0.9161\n",
            "Epoch 204/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6388 - accuracy: 0.9183\n",
            "Epoch 205/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.6356 - accuracy: 0.9183\n",
            "Epoch 206/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6536 - accuracy: 0.9117\n",
            "Epoch 207/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6458 - accuracy: 0.9139\n",
            "Epoch 208/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6446 - accuracy: 0.9227\n",
            "Epoch 209/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6382 - accuracy: 0.9183\n",
            "Epoch 210/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6447 - accuracy: 0.9161\n",
            "Epoch 211/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.6240 - accuracy: 0.9183\n",
            "Epoch 212/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.6065 - accuracy: 0.9227\n",
            "Epoch 213/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5962 - accuracy: 0.9294\n",
            "Epoch 214/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5853 - accuracy: 0.9272\n",
            "Epoch 215/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5788 - accuracy: 0.9249\n",
            "Epoch 216/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5783 - accuracy: 0.9249\n",
            "Epoch 217/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5725 - accuracy: 0.9249\n",
            "Epoch 218/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5629 - accuracy: 0.9294\n",
            "Epoch 219/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5577 - accuracy: 0.9272\n",
            "Epoch 220/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5526 - accuracy: 0.9294\n",
            "Epoch 221/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5466 - accuracy: 0.9316\n",
            "Epoch 222/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5403 - accuracy: 0.9338\n",
            "Epoch 223/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5347 - accuracy: 0.9338\n",
            "Epoch 224/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5377 - accuracy: 0.9249\n",
            "Epoch 225/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5510 - accuracy: 0.9183\n",
            "Epoch 226/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5448 - accuracy: 0.9161\n",
            "Epoch 227/500\n",
            "15/15 [==============================] - 0s 12ms/step - loss: 0.5410 - accuracy: 0.9249\n",
            "Epoch 228/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5326 - accuracy: 0.9249\n",
            "Epoch 229/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5238 - accuracy: 0.9249\n",
            "Epoch 230/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.5140 - accuracy: 0.9249\n",
            "Epoch 231/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5426 - accuracy: 0.9139\n",
            "Epoch 232/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5351 - accuracy: 0.9227\n",
            "Epoch 233/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.5198 - accuracy: 0.9272\n",
            "Epoch 234/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.5010 - accuracy: 0.9338\n",
            "Epoch 235/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4900 - accuracy: 0.9338\n",
            "Epoch 236/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4797 - accuracy: 0.9404\n",
            "Epoch 237/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4723 - accuracy: 0.9382\n",
            "Epoch 238/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4667 - accuracy: 0.9360\n",
            "Epoch 239/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4624 - accuracy: 0.9360\n",
            "Epoch 240/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.4581 - accuracy: 0.9360\n",
            "Epoch 241/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4543 - accuracy: 0.9382\n",
            "Epoch 242/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4501 - accuracy: 0.9382\n",
            "Epoch 243/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4472 - accuracy: 0.9404\n",
            "Epoch 244/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4419 - accuracy: 0.9404\n",
            "Epoch 245/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4376 - accuracy: 0.9404\n",
            "Epoch 246/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.4340 - accuracy: 0.9404\n",
            "Epoch 247/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4301 - accuracy: 0.9404\n",
            "Epoch 248/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4263 - accuracy: 0.9404\n",
            "Epoch 249/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4220 - accuracy: 0.9426\n",
            "Epoch 250/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.4188 - accuracy: 0.9404\n",
            "Epoch 251/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4151 - accuracy: 0.9404\n",
            "Epoch 252/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4119 - accuracy: 0.9404\n",
            "Epoch 253/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4095 - accuracy: 0.9426\n",
            "Epoch 254/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4057 - accuracy: 0.9426\n",
            "Epoch 255/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.4022 - accuracy: 0.9426\n",
            "Epoch 256/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.3986 - accuracy: 0.9426\n",
            "Epoch 257/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3962 - accuracy: 0.9404\n",
            "Epoch 258/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.3936 - accuracy: 0.9426\n",
            "Epoch 259/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3902 - accuracy: 0.9426\n",
            "Epoch 260/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3860 - accuracy: 0.9448\n",
            "Epoch 261/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3829 - accuracy: 0.9470\n",
            "Epoch 262/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3800 - accuracy: 0.9470\n",
            "Epoch 263/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3767 - accuracy: 0.9448\n",
            "Epoch 264/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3744 - accuracy: 0.9448\n",
            "Epoch 265/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3725 - accuracy: 0.9426\n",
            "Epoch 266/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3683 - accuracy: 0.9448\n",
            "Epoch 267/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3654 - accuracy: 0.9470\n",
            "Epoch 268/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3628 - accuracy: 0.9492\n",
            "Epoch 269/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3598 - accuracy: 0.9448\n",
            "Epoch 270/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3558 - accuracy: 0.9492\n",
            "Epoch 271/500\n",
            "15/15 [==============================] - 0s 20ms/step - loss: 0.3540 - accuracy: 0.9492\n",
            "Epoch 272/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3529 - accuracy: 0.9492\n",
            "Epoch 273/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3486 - accuracy: 0.9492\n",
            "Epoch 274/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3463 - accuracy: 0.9492\n",
            "Epoch 275/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3428 - accuracy: 0.9492\n",
            "Epoch 276/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3403 - accuracy: 0.9492\n",
            "Epoch 277/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3378 - accuracy: 0.9492\n",
            "Epoch 278/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3364 - accuracy: 0.9492\n",
            "Epoch 279/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3338 - accuracy: 0.9492\n",
            "Epoch 280/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3317 - accuracy: 0.9492\n",
            "Epoch 281/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3305 - accuracy: 0.9448\n",
            "Epoch 282/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3283 - accuracy: 0.9470\n",
            "Epoch 283/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3254 - accuracy: 0.9426\n",
            "Epoch 284/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3224 - accuracy: 0.9470\n",
            "Epoch 285/500\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.3193 - accuracy: 0.9470\n",
            "Epoch 286/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3173 - accuracy: 0.9492\n",
            "Epoch 287/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3162 - accuracy: 0.9492\n",
            "Epoch 288/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.4032 - accuracy: 0.9382\n",
            "Epoch 289/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.4111 - accuracy: 0.9338\n",
            "Epoch 290/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3831 - accuracy: 0.9382\n",
            "Epoch 291/500\n",
            "15/15 [==============================] - 0s 16ms/step - loss: 0.3560 - accuracy: 0.9404\n",
            "Epoch 292/500\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.3625 - accuracy: 0.9382\n",
            "Epoch 293/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3404 - accuracy: 0.9448\n",
            "Epoch 294/500\n",
            "15/15 [==============================] - 0s 19ms/step - loss: 0.3404 - accuracy: 0.9426\n",
            "Epoch 295/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3303 - accuracy: 0.9448\n",
            "Epoch 296/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3224 - accuracy: 0.9448\n",
            "Epoch 297/500\n",
            "15/15 [==============================] - 0s 15ms/step - loss: 0.3134 - accuracy: 0.9448\n",
            "Epoch 298/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.3064 - accuracy: 0.9470\n",
            "Epoch 299/500\n",
            "15/15 [==============================] - 0s 18ms/step - loss: 0.3030 - accuracy: 0.9470\n",
            "Epoch 300/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.2988 - accuracy: 0.9492\n",
            "Epoch 301/500\n",
            "15/15 [==============================] - 0s 17ms/step - loss: 0.2945 - accuracy: 0.9492\n",
            "Epoch 302/500\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2924 - accuracy: 0.9492\n",
            "Epoch 303/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2890 - accuracy: 0.9470\n",
            "Epoch 304/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2935 - accuracy: 0.9448\n",
            "Epoch 305/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2875 - accuracy: 0.9470\n",
            "Epoch 306/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2843 - accuracy: 0.9514\n",
            "Epoch 307/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2812 - accuracy: 0.9492\n",
            "Epoch 308/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2790 - accuracy: 0.9492\n",
            "Epoch 309/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2803 - accuracy: 0.9492\n",
            "Epoch 310/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2760 - accuracy: 0.9514\n",
            "Epoch 311/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2730 - accuracy: 0.9514\n",
            "Epoch 312/500\n",
            "15/15 [==============================] - 0s 7ms/step - loss: 0.2705 - accuracy: 0.9492\n",
            "Epoch 313/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2691 - accuracy: 0.9492\n",
            "Epoch 314/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2664 - accuracy: 0.9514\n",
            "Epoch 315/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2645 - accuracy: 0.9492\n",
            "Epoch 316/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2630 - accuracy: 0.9514\n",
            "Epoch 317/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2606 - accuracy: 0.9514\n",
            "Epoch 318/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2594 - accuracy: 0.9492\n",
            "Epoch 319/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2579 - accuracy: 0.9470\n",
            "Epoch 320/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2562 - accuracy: 0.9492\n",
            "Epoch 321/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2544 - accuracy: 0.9492\n",
            "Epoch 322/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2530 - accuracy: 0.9514\n",
            "Epoch 323/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2513 - accuracy: 0.9492\n",
            "Epoch 324/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2496 - accuracy: 0.9492\n",
            "Epoch 325/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2485 - accuracy: 0.9492\n",
            "Epoch 326/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2482 - accuracy: 0.9448\n",
            "Epoch 327/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2463 - accuracy: 0.9470\n",
            "Epoch 328/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2447 - accuracy: 0.9514\n",
            "Epoch 329/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2436 - accuracy: 0.9492\n",
            "Epoch 330/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2418 - accuracy: 0.9492\n",
            "Epoch 331/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2393 - accuracy: 0.9492\n",
            "Epoch 332/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.2380 - accuracy: 0.9514\n",
            "Epoch 333/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2372 - accuracy: 0.9470\n",
            "Epoch 334/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2357 - accuracy: 0.9492\n",
            "Epoch 335/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2344 - accuracy: 0.9492\n",
            "Epoch 336/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2347 - accuracy: 0.9470\n",
            "Epoch 337/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2330 - accuracy: 0.9470\n",
            "Epoch 338/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2312 - accuracy: 0.9492\n",
            "Epoch 339/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2315 - accuracy: 0.9448\n",
            "Epoch 340/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2288 - accuracy: 0.9514\n",
            "Epoch 341/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.2276 - accuracy: 0.9514\n",
            "Epoch 342/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2265 - accuracy: 0.9492\n",
            "Epoch 343/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2252 - accuracy: 0.9492\n",
            "Epoch 344/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2242 - accuracy: 0.9470\n",
            "Epoch 345/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2228 - accuracy: 0.9514\n",
            "Epoch 346/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2222 - accuracy: 0.9470\n",
            "Epoch 347/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2213 - accuracy: 0.9514\n",
            "Epoch 348/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2192 - accuracy: 0.9514\n",
            "Epoch 349/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2178 - accuracy: 0.9514\n",
            "Epoch 350/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2197 - accuracy: 0.9514\n",
            "Epoch 351/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2204 - accuracy: 0.9514\n",
            "Epoch 352/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2154 - accuracy: 0.9492\n",
            "Epoch 353/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2140 - accuracy: 0.9514\n",
            "Epoch 354/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2138 - accuracy: 0.9514\n",
            "Epoch 355/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2134 - accuracy: 0.9470\n",
            "Epoch 356/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.2112 - accuracy: 0.9514\n",
            "Epoch 357/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2103 - accuracy: 0.9492\n",
            "Epoch 358/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2087 - accuracy: 0.9448\n",
            "Epoch 359/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2076 - accuracy: 0.9492\n",
            "Epoch 360/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2059 - accuracy: 0.9514\n",
            "Epoch 361/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2045 - accuracy: 0.9492\n",
            "Epoch 362/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2041 - accuracy: 0.9536\n",
            "Epoch 363/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2040 - accuracy: 0.9536\n",
            "Epoch 364/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2013 - accuracy: 0.9514\n",
            "Epoch 365/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2017 - accuracy: 0.9514\n",
            "Epoch 366/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1998 - accuracy: 0.9536\n",
            "Epoch 367/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.2020 - accuracy: 0.9492\n",
            "Epoch 368/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1996 - accuracy: 0.9470\n",
            "Epoch 369/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1979 - accuracy: 0.9492\n",
            "Epoch 370/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1974 - accuracy: 0.9492\n",
            "Epoch 371/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1956 - accuracy: 0.9536\n",
            "Epoch 372/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1956 - accuracy: 0.9514\n",
            "Epoch 373/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1942 - accuracy: 0.9470\n",
            "Epoch 374/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1940 - accuracy: 0.9536\n",
            "Epoch 375/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1941 - accuracy: 0.9536\n",
            "Epoch 376/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1919 - accuracy: 0.9492\n",
            "Epoch 377/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1899 - accuracy: 0.9514\n",
            "Epoch 378/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1886 - accuracy: 0.9514\n",
            "Epoch 379/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1871 - accuracy: 0.9492\n",
            "Epoch 380/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1866 - accuracy: 0.9426\n",
            "Epoch 381/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1852 - accuracy: 0.9514\n",
            "Epoch 382/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1852 - accuracy: 0.9514\n",
            "Epoch 383/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1839 - accuracy: 0.9536\n",
            "Epoch 384/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1828 - accuracy: 0.9514\n",
            "Epoch 385/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1824 - accuracy: 0.9536\n",
            "Epoch 386/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1810 - accuracy: 0.9514\n",
            "Epoch 387/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1811 - accuracy: 0.9514\n",
            "Epoch 388/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1804 - accuracy: 0.9492\n",
            "Epoch 389/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1795 - accuracy: 0.9470\n",
            "Epoch 390/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1781 - accuracy: 0.9470\n",
            "Epoch 391/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1771 - accuracy: 0.9514\n",
            "Epoch 392/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1766 - accuracy: 0.9470\n",
            "Epoch 393/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1764 - accuracy: 0.9448\n",
            "Epoch 394/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1752 - accuracy: 0.9470\n",
            "Epoch 395/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1745 - accuracy: 0.9536\n",
            "Epoch 396/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1735 - accuracy: 0.9492\n",
            "Epoch 397/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1755 - accuracy: 0.9536\n",
            "Epoch 398/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1882 - accuracy: 0.9448\n",
            "Epoch 399/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1902 - accuracy: 0.9448\n",
            "Epoch 400/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1807 - accuracy: 0.9426\n",
            "Epoch 401/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1748 - accuracy: 0.9514\n",
            "Epoch 402/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1721 - accuracy: 0.9514\n",
            "Epoch 403/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1730 - accuracy: 0.9448\n",
            "Epoch 404/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1696 - accuracy: 0.9514\n",
            "Epoch 405/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1702 - accuracy: 0.9492\n",
            "Epoch 406/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1682 - accuracy: 0.9514\n",
            "Epoch 407/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1673 - accuracy: 0.9514\n",
            "Epoch 408/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1663 - accuracy: 0.9492\n",
            "Epoch 409/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1655 - accuracy: 0.9536\n",
            "Epoch 410/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1653 - accuracy: 0.9514\n",
            "Epoch 411/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1647 - accuracy: 0.9514\n",
            "Epoch 412/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1641 - accuracy: 0.9514\n",
            "Epoch 413/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1631 - accuracy: 0.9514\n",
            "Epoch 414/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1623 - accuracy: 0.9514\n",
            "Epoch 415/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1621 - accuracy: 0.9514\n",
            "Epoch 416/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1623 - accuracy: 0.9492\n",
            "Epoch 417/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1612 - accuracy: 0.9514\n",
            "Epoch 418/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1600 - accuracy: 0.9470\n",
            "Epoch 419/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1597 - accuracy: 0.9470\n",
            "Epoch 420/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1591 - accuracy: 0.9514\n",
            "Epoch 421/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1590 - accuracy: 0.9448\n",
            "Epoch 422/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1578 - accuracy: 0.9448\n",
            "Epoch 423/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.9470\n",
            "Epoch 424/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1564 - accuracy: 0.9492\n",
            "Epoch 425/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1556 - accuracy: 0.9514\n",
            "Epoch 426/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.9536\n",
            "Epoch 427/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1558 - accuracy: 0.9514\n",
            "Epoch 428/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1551 - accuracy: 0.9536\n",
            "Epoch 429/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1534 - accuracy: 0.9448\n",
            "Epoch 430/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1529 - accuracy: 0.9470\n",
            "Epoch 431/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1527 - accuracy: 0.9536\n",
            "Epoch 432/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1524 - accuracy: 0.9536\n",
            "Epoch 433/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1518 - accuracy: 0.9536\n",
            "Epoch 434/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1508 - accuracy: 0.9470\n",
            "Epoch 435/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1501 - accuracy: 0.9514\n",
            "Epoch 436/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1502 - accuracy: 0.9514\n",
            "Epoch 437/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1493 - accuracy: 0.9470\n",
            "Epoch 438/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1491 - accuracy: 0.9404\n",
            "Epoch 439/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1482 - accuracy: 0.9492\n",
            "Epoch 440/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1474 - accuracy: 0.9492\n",
            "Epoch 441/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1472 - accuracy: 0.9492\n",
            "Epoch 442/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9470\n",
            "Epoch 443/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1465 - accuracy: 0.9492\n",
            "Epoch 444/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1487 - accuracy: 0.9448\n",
            "Epoch 445/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1495 - accuracy: 0.9448\n",
            "Epoch 446/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1484 - accuracy: 0.9470\n",
            "Epoch 447/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9514\n",
            "Epoch 448/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1452 - accuracy: 0.9514\n",
            "Epoch 449/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1448 - accuracy: 0.9492\n",
            "Epoch 450/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1438 - accuracy: 0.9514\n",
            "Epoch 451/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1461 - accuracy: 0.9470\n",
            "Epoch 452/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1449 - accuracy: 0.9492\n",
            "Epoch 453/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1442 - accuracy: 0.9492\n",
            "Epoch 454/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1427 - accuracy: 0.9514\n",
            "Epoch 455/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1421 - accuracy: 0.9470\n",
            "Epoch 456/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1428 - accuracy: 0.9514\n",
            "Epoch 457/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1422 - accuracy: 0.9536\n",
            "Epoch 458/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1424 - accuracy: 0.9492\n",
            "Epoch 459/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1415 - accuracy: 0.9470\n",
            "Epoch 460/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1398 - accuracy: 0.9536\n",
            "Epoch 461/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1393 - accuracy: 0.9514\n",
            "Epoch 462/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1384 - accuracy: 0.9470\n",
            "Epoch 463/500\n",
            "15/15 [==============================] - 0s 8ms/step - loss: 0.1381 - accuracy: 0.9448\n",
            "Epoch 464/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1378 - accuracy: 0.9448\n",
            "Epoch 465/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1377 - accuracy: 0.9470\n",
            "Epoch 466/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1364 - accuracy: 0.9470\n",
            "Epoch 467/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1369 - accuracy: 0.9492\n",
            "Epoch 468/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1364 - accuracy: 0.9492\n",
            "Epoch 469/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1361 - accuracy: 0.9470\n",
            "Epoch 470/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1362 - accuracy: 0.9448\n",
            "Epoch 471/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1351 - accuracy: 0.9448\n",
            "Epoch 472/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1344 - accuracy: 0.9514\n",
            "Epoch 473/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1347 - accuracy: 0.9492\n",
            "Epoch 474/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1353 - accuracy: 0.9514\n",
            "Epoch 475/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1340 - accuracy: 0.9470\n",
            "Epoch 476/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1347 - accuracy: 0.9492\n",
            "Epoch 477/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1345 - accuracy: 0.9448\n",
            "Epoch 478/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1332 - accuracy: 0.9514\n",
            "Epoch 479/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1328 - accuracy: 0.9470\n",
            "Epoch 480/500\n",
            "15/15 [==============================] - 0s 11ms/step - loss: 0.1319 - accuracy: 0.9492\n",
            "Epoch 481/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1317 - accuracy: 0.9404\n",
            "Epoch 482/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1318 - accuracy: 0.9470\n",
            "Epoch 483/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1310 - accuracy: 0.9514\n",
            "Epoch 484/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1307 - accuracy: 0.9492\n",
            "Epoch 485/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1308 - accuracy: 0.9448\n",
            "Epoch 486/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1300 - accuracy: 0.9492\n",
            "Epoch 487/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1296 - accuracy: 0.9492\n",
            "Epoch 488/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1296 - accuracy: 0.9514\n",
            "Epoch 489/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1290 - accuracy: 0.9470\n",
            "Epoch 490/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1286 - accuracy: 0.9470\n",
            "Epoch 491/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1280 - accuracy: 0.9470\n",
            "Epoch 492/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1286 - accuracy: 0.9426\n",
            "Epoch 493/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1282 - accuracy: 0.9492\n",
            "Epoch 494/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1279 - accuracy: 0.9470\n",
            "Epoch 495/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1273 - accuracy: 0.9514\n",
            "Epoch 496/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1275 - accuracy: 0.9492\n",
            "Epoch 497/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1270 - accuracy: 0.9448\n",
            "Epoch 498/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9448\n",
            "Epoch 499/500\n",
            "15/15 [==============================] - 0s 10ms/step - loss: 0.1260 - accuracy: 0.9492\n",
            "Epoch 500/500\n",
            "15/15 [==============================] - 0s 9ms/step - loss: 0.1263 - accuracy: 0.9448\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZzPf_OJRnPK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D_AcO1mRu4o",
        "outputId": "8d2ad7c6-7d66-4d2a-9fbf-a214f57055a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQCxF6GfR4Qr",
        "outputId": "4c59178c-f571-4cd5-94de-b13ddb41a4e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seed_text = \"Sissi went to London\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sissi went to London as red as a call one introduction pipes away round entangled entangled glisten entangled glisten replied round round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone away round away mavrone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kswiEwxOSLFB"
      },
      "source": [
        "### Part II\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgTkzNqYSURz"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjwGcqmYSo4c",
        "outputId": "b550157f-5dfc-4a06-e01f-ff2a0b77b9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt \\\n",
        "    -O /tmp/irish-lyrics-eof.txt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-08 22:32:20--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/irish-lyrics-eof.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.142.128, 74.125.195.128, 74.125.20.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.142.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68970 (67K) [text/plain]\n",
            "Saving to: /tmp/irish-lyrics-eof.txt\n",
            "\n",
            "\r          /tmp/iris   0%[                    ]       0  --.-KB/s               \r/tmp/irish-lyrics-e 100%[===================>]  67.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-10-08 22:32:20 (158 MB/s) - /tmp/irish-lyrics-eof.txt saved [68970/68970]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dGCMnUpSqNa",
        "outputId": "d474b63e-84f5-40a0-85fa-9e92e4270282",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = open('/tmp/irish-lyrics-eof.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'and': 2, 'i': 3, 'to': 4, 'a': 5, 'of': 6, 'my': 7, 'in': 8, 'me': 9, 'for': 10, 'you': 11, 'all': 12, 'was': 13, 'she': 14, 'that': 15, 'on': 16, 'with': 17, 'her': 18, 'but': 19, 'as': 20, 'when': 21, 'love': 22, 'is': 23, 'your': 24, 'it': 25, 'will': 26, 'from': 27, 'by': 28, 'they': 29, 'be': 30, 'are': 31, 'so': 32, 'he': 33, 'old': 34, 'no': 35, 'oh': 36, 'ill': 37, 'at': 38, 'one': 39, 'his': 40, 'there': 41, 'were': 42, 'heart': 43, 'down': 44, 'now': 45, 'we': 46, 'where': 47, 'young': 48, 'never': 49, 'go': 50, 'come': 51, 'then': 52, 'did': 53, 'not': 54, 'said': 55, 'away': 56, 'their': 57, 'sweet': 58, 'them': 59, 'green': 60, 'if': 61, 'take': 62, 'our': 63, 'like': 64, 'night': 65, 'day': 66, 'o': 67, 'out': 68, 'fair': 69, 'this': 70, 'town': 71, 'have': 72, 'can': 73, 'true': 74, 'its': 75, 'thou': 76, 'see': 77, 'dear': 78, 'more': 79, 'theres': 80, 'or': 81, 'had': 82, 'would': 83, 'over': 84, 'hear': 85, 'up': 86, 'ive': 87, 'through': 88, 'home': 89, 'again': 90, 'well': 91, 'oer': 92, 'land': 93, 'good': 94, 'im': 95, 'ye': 96, 'sea': 97, 'left': 98, 'still': 99, 'father': 100, 'long': 101, 'rose': 102, 'could': 103, 'morning': 104, 'wild': 105, 'who': 106, 'eyes': 107, 'came': 108, 'while': 109, 'too': 110, 'back': 111, 'little': 112, 'an': 113, 'took': 114, 'him': 115, 'bow': 116, 'first': 117, 'let': 118, 'man': 119, 'shall': 120, 'know': 121, 'get': 122, 'high': 123, 'gone': 124, 'say': 125, 'ever': 126, 'some': 127, 'mary': 128, 'hand': 129, 'till': 130, 'put': 131, 'own': 132, 'time': 133, 'heard': 134, 'dead': 135, 'may': 136, 'bright': 137, 'mountain': 138, 'early': 139, 'rosin': 140, 'gave': 141, 'thee': 142, 'only': 143, 'far': 144, 'maid': 145, 'must': 146, 'find': 147, 'girl': 148, 'sure': 149, 'round': 150, 'dublin': 151, 'once': 152, 'world': 153, 'delight': 154, 'last': 155, 'johnny': 156, 'seen': 157, 'has': 158, 'fine': 159, 'road': 160, 'mother': 161, 'tis': 162, 'what': 163, 'way': 164, 'moon': 165, 'soul': 166, 'neer': 167, 'id': 168, 'just': 169, 'thats': 170, 'days': 171, 'darling': 172, 'went': 173, 'white': 174, 'die': 175, 'than': 176, 'hair': 177, 'goes': 178, 'meet': 179, 'today': 180, 'do': 181, 'girls': 182, 'shes': 183, 'thyme': 184, 'thy': 185, 'sing': 186, 'pretty': 187, 'new': 188, 'poor': 189, 'into': 190, 'life': 191, 'irish': 192, 'give': 193, 'boy': 194, 'youre': 195, 'make': 196, 'passed': 197, 'lovely': 198, 'black': 199, 'youll': 200, 'died': 201, 'red': 202, 'smile': 203, 'keep': 204, 'loves': 205, 'free': 206, 'leave': 207, 'friends': 208, 'each': 209, 'saw': 210, 'behind': 211, 'song': 212, 'ra': 213, 'dont': 214, 'arms': 215, 'am': 216, 'sun': 217, 'saying': 218, 'made': 219, 'wish': 220, 'cold': 221, 'met': 222, 'before': 223, 'should': 224, 'rocky': 225, 'light': 226, 'wid': 227, 'boys': 228, 'best': 229, 'fields': 230, 'since': 231, 'ball': 232, 'water': 233, 'casey': 234, 'mind': 235, 'along': 236, 'loved': 237, 'place': 238, 'ireland': 239, 'next': 240, 'three': 241, 'many': 242, 'years': 243, 'door': 244, 'us': 245, 'drink': 246, 'got': 247, 'might': 248, 'live': 249, 'roses': 250, 'play': 251, 'soon': 252, 'ground': 253, 'times': 254, 'spent': 255, 'going': 256, 'tree': 257, 'barley': 258, 'grass': 259, 'kind': 260, 'twas': 261, 'bridge': 262, 'around': 263, 'blue': 264, 'tell': 265, 'row': 266, 'how': 267, 'money': 268, 'merry': 269, 'stepped': 270, 'corporal': 271, 'always': 272, 'though': 273, 'near': 274, 'taken': 275, 'ones': 276, 'daughter': 277, 'forever': 278, 'loo': 279, 'shining': 280, 'plenty': 281, 'hes': 282, 'ship': 283, 'banks': 284, 'think': 285, 'very': 286, 'stand': 287, 'heres': 288, 'snow': 289, 'mountains': 290, 'molly': 291, 'wheel': 292, 'street': 293, 'erin': 294, 'side': 295, 'feet': 296, 'star': 297, 'look': 298, 'brave': 299, 'woman': 300, 'sons': 301, 'two': 302, 'says': 303, 'asked': 304, 'lanigans': 305, 'singing': 306, 'men': 307, 'toome': 308, 'stole': 309, 'god': 310, 'hill': 311, 'lonely': 312, 'lover': 313, 'tears': 314, 'fathers': 315, 'low': 316, 'voice': 317, 'quite': 318, 'able': 319, 'nice': 320, 'laid': 321, 'comrades': 322, 'wind': 323, 'another': 324, 'sit': 325, 'face': 326, 'band': 327, 'call': 328, 'colleen': 329, 'until': 330, 'hills': 331, 'mine': 332, 'above': 333, 'upon': 334, 'eer': 335, 'youve': 336, 'fly': 337, 'been': 338, 'late': 339, 'alive': 340, 'ballyjamesduff': 341, 'looked': 342, 'great': 343, 'why': 344, 'every': 345, 'proud': 346, 'found': 347, 'bragh': 348, 'such': 349, 'birds': 350, 'wedding': 351, 'welcome': 352, 'dancing': 353, 'da': 354, 'fell': 355, 'thinking': 356, 'roddy': 357, 'mccorley': 358, 'smiling': 359, 'mallow': 360, 'blooming': 361, 'thought': 362, 'peace': 363, 'soft': 364, 'pure': 365, 'harp': 366, 'dream': 367, 'alas': 368, 'yet': 369, 'clear': 370, 'art': 371, 'off': 372, 'hope': 373, 'fought': 374, 'mothers': 375, 'shore': 376, 'ago': 377, 'fol': 378, 'de': 379, 'house': 380, 'married': 381, 'bound': 382, 'danced': 383, 'devil': 384, 'dawning': 385, 'makes': 386, 'same': 387, 'sat': 388, 'any': 389, 'glass': 390, 'gay': 391, 'relations': 392, 'evening': 393, 'watched': 394, 'right': 395, 'fellows': 396, 'whiskey': 397, 'bonnie': 398, 'grows': 399, 'women': 400, 'flowers': 401, 'beauty': 402, 'cannot': 403, 'handsome': 404, 'happy': 405, 'gold': 406, 'rover': 407, 'none': 408, 'doneen': 409, 'summers': 410, 'people': 411, 'set': 412, 'paddy': 413, 'morn': 414, 'most': 415, 'easy': 416, 'struck': 417, 'beautiful': 418, 'those': 419, 'golden': 420, 'run': 421, 'pipes': 422, 'glen': 423, 'dying': 424, 'here': 425, 'wall': 426, 'across': 427, 'fire': 428, 'eileen': 429, 'longer': 430, 'cheeks': 431, 'valley': 432, 'both': 433, 'dew': 434, 'care': 435, 'bride': 436, 'nothing': 437, 'wont': 438, 'theyre': 439, 'colonel': 440, 'maiden': 441, 'shed': 442, 'til': 443, 'brown': 444, 'breast': 445, 'corn': 446, 'sinking': 447, 'began': 448, 'name': 449, 'cruel': 450, 'sound': 451, 'spancil': 452, 'county': 453, 'lies': 454, 'color': 455, 'thing': 456, 'decay': 457, 'sleep': 458, 'hours': 459, 'loving': 460, 'weary': 461, 'ringing': 462, 'please': 463, 'forget': 464, 'lie': 465, 'ran': 466, 'tore': 467, 'country': 468, 'fear': 469, 'fortune': 470, 'kissed': 471, 'alone': 472, 'ould': 473, 'cry': 474, 'dreams': 475, 'used': 476, 'horse': 477, 'break': 478, 'bells': 479, 'didnt': 480, 'weeks': 481, 'without': 482, 'raw': 483, 'nor': 484, 'twenty': 485, 'tune': 486, 'hed': 487, 'roving': 488, 'leaves': 489, 'cant': 490, 'death': 491, 'ten': 492, 'prison': 493, 'judge': 494, 'against': 495, 'lads': 496, 'shell': 497, 'fill': 498, 'valleys': 499, 'other': 500, 'pale': 501, 'joy': 502, 'wide': 503, 'bring': 504, 'ah': 505, 'cliffs': 506, 'city': 507, 'end': 508, 'turn': 509, 'sky': 510, 'born': 511, 'knew': 512, 'smiled': 513, 'rosie': 514, 'comes': 515, 'sayin': 516, 'lord': 517, 'dungannon': 518, 'blood': 519, 'air': 520, 'danny': 521, 'calling': 522, 'sunshine': 523, 'spring': 524, 'bid': 525, 'grow': 526, 'truth': 527, 'tear': 528, 'rings': 529, 'guns': 530, 'bay': 531, 'oflynn': 532, 'och': 533, 'stick': 534, 'rest': 535, 'four': 536, 'jewel': 537, 'tried': 538, 'grief': 539, 'answer': 540, 'kathleen': 541, 'fond': 542, 'eye': 543, 'goin': 544, 'pistols': 545, 'musha': 546, 'whack': 547, 'creole': 548, 'together': 549, 'room': 550, 'fall': 551, 'swore': 552, 'being': 553, 'step': 554, 'lark': 555, 'cailn': 556, 'deas': 557, 'crite': 558, 'na': 559, 'mb': 560, 'sir': 561, 'isle': 562, 'waiting': 563, 'magic': 564, 'skibbereen': 565, 'loud': 566, 'raise': 567, 'bent': 568, 'aged': 569, 'summer': 570, 'jenny': 571, 'excise': 572, 'rigadoo': 573, 'auld': 574, 'hearts': 575, 'nay': 576, 'stool': 577, 'farrell': 578, 'garden': 579, 'precious': 580, 'child': 581, 'slumber': 582, 'sleeping': 583, 'watch': 584, 'gently': 585, 'minstrel': 586, 'praise': 587, 'bell': 588, 'shaken': 589, 'immortal': 590, 'pray': 591, 'stay': 592, 'spoke': 593, 'cross': 594, 'brothers': 595, 'much': 596, 'past': 597, 'killarney': 598, 'sang': 599, 'tones': 600, 'ral': 601, 'wander': 602, 'cot': 603, 'feel': 604, 'yore': 605, 'answered': 606, 'divil': 607, 'middle': 608, 'bit': 609, 'led': 610, 'soldiers': 611, 'lily': 612, 'bed': 613, 'lassie': 614, 'clothes': 615, 'return': 616, 'broken': 617, 'derry': 618, 'sighed': 619, 'english': 620, 'tomorrow': 621, 'souls': 622, 'van': 623, 'diemans': 624, 'law': 625, 'neither': 626, 'winds': 627, 'rather': 628, 'doesnt': 629, 'rosy': 630, 'neatest': 631, 'hands': 632, 'whereon': 633, 'stands': 634, 'write': 635, 'thousand': 636, 'fare': 637, 'youd': 638, 'velvet': 639, 'neat': 640, 'landed': 641, 'health': 642, 'kellswater': 643, 'quiet': 644, 'stars': 645, 'beside': 646, 'warm': 647, 'sunday': 648, 'grey': 649, 'ocean': 650, 'sad': 651, 'spend': 652, 'kilkenny': 653, 'silver': 654, 'view': 655, 'west': 656, 'plain': 657, 'barrow': 658, 'broad': 659, 'narrow': 660, 'crying': 661, 'wonder': 662, 'save': 663, 'stop': 664, 'tender': 665, 'told': 666, 'lip': 667, 'dance': 668, 'foot': 669, 'kilrain': 670, 'saint': 671, 'visit': 672, 'mossy': 673, 'wexford': 674, 'irishmen': 675, 'shadow': 676, 'tho': 677, 'salley': 678, 'gardens': 679, 'foolish': 680, 'youth': 681, 'fade': 682, 'war': 683, 'believe': 684, 'which': 685, 'change': 686, 'entwine': 687, 'turns': 688, 'turned': 689, 'crown': 690, 'played': 691, 'captain': 692, 'blow': 693, 'children': 694, 'slainte': 695, 'gentle': 696, 'heavens': 697, 'bloom': 698, 'grand': 699, 'bush': 700, 'nest': 701, 'rich': 702, 'parting': 703, 'better': 704, 'window': 705, 'haste': 706, 'fresh': 707, 'stream': 708, 'rays': 709, 'ma': 710, 'ring': 711, 'lad': 712, 'athy': 713, 'drop': 714, 'hardly': 715, 'done': 716, 'arm': 717, 'leg': 718, 'beg': 719, 'drew': 720, 'bold': 721, 'drawn': 722, 'jail': 723, 'writin': 724, 'farewell': 725, 'tired': 726, 'lake': 727, 'want': 728, 'ringlets': 729, 'myself': 730, 'songs': 731, 'reel': 732, 'steps': 733, 'hearty': 734, 'fainted': 735, 'called': 736, 'under': 737, 'toe': 738, 'mairi': 739, 'fairest': 740, 'darlin': 741, 'bird': 742, 'memory': 743, 'lips': 744, 'sweetly': 745, 'morrow': 746, 'consent': 747, 'else': 748, 'sold': 749, 'stout': 750, 'pair': 751, 'drinking': 752, 'meself': 753, 'fray': 754, 'pike': 755, 'coat': 756, 'beneath': 757, 'rent': 758, 'part': 759, 'half': 760, 'head': 761, 'friend': 762, 'standing': 763, 'floor': 764, 'bare': 765, 'wed': 766, 'son': 767, 'pride': 768, 'vision': 769, 'sword': 770, 'after': 771, 'won': 772, 'farmers': 773, 'flower': 774, 'nut': 775, 'surely': 776, 'stood': 777, 'wandered': 778, 'athenry': 779, 'rising': 780, 'beating': 781, 'form': 782, 'dhu': 783, 'buy': 784, 'laughter': 785, 'wear': 786, 'raking': 787, 'rakes': 788, 'claret': 789, 'shure': 790, 'tralee': 791, 'slower': 792, 'lower': 793, 'deep': 794, 'wearin': 795, 'duram': 796, 'takes': 797, 'beware': 798, 'steal': 799, 'brings': 800, 'things': 801, 'joys': 802, 'bunch': 803, 'sailor': 804, 'chanced': 805, 'pass': 806, 'angels': 807, 'send': 808, 'drowsy': 809, 'keeping': 810, 'spirit': 811, 'stealing': 812, 'feeling': 813, 'roam': 814, 'presence': 815, 'heavenward': 816, 'dust': 817, 'dim': 818, 'journey': 819, 'waves': 820, 'frightened': 821, 'leaving': 822, 'struggle': 823, 'parents': 824, 'courage': 825, 'weeping': 826, 'pain': 827, 'mist': 828, 'felt': 829, 'roared': 830, 'making': 831, 'fever': 832, 'moment': 833, 'distance': 834, 'wailing': 835, 'oft': 836, 'held': 837, 'fast': 838, 'cabin': 839, 'honey': 840, 'diddle': 841, 'clearly': 842, 'open': 843, 'opened': 844, 'table': 845, 'wine': 846, 'lay': 847, 'shells': 848, 'sailed': 849, 'drown': 850, 'fetters': 851, 'chains': 852, 'wives': 853, 'sorrow': 854, 'thoughts': 855, 'cursed': 856, 'hell': 857, 'five': 858, 'buried': 859, 'lost': 860, 'endless': 861, 'slavery': 862, 'gun': 863, 'rain': 864, 'cares': 865, 'ghosts': 866, 'runaway': 867, 'twill': 868, 'month': 869, 'meadows': 870, 'prettiest': 871, 'winters': 872, 'satisfied': 873, 'few': 874, 'short': 875, 'lines': 876, 'shone': 877, 'shoulder': 878, 'belfast': 879, 'trade': 880, 'bad': 881, 'caused': 882, 'stray': 883, 'meaning': 884, 'damsel': 885, 'appear': 886, 'seven': 887, 'sentence': 888, 'jolly': 889, 'whenever': 890, 'wee': 891, 'wife': 892, 'lives': 893, 'martha': 894, 'courted': 895, 'bridgit': 896, 'omalley': 897, 'desolation': 898, 'thorn': 899, 'gaze': 900, 'stone': 901, 'approaching': 902, 'sets': 903, 'carrigfergus': 904, 'nights': 905, 'swim': 906, 'wings': 907, 'sober': 908, 'travel': 909, 'native': 910, 'places': 911, 'slopes': 912, 'hares': 913, 'lofty': 914, 'malone': 915, 'wheeled': 916, 'streets': 917, 'enough': 918, 'reilly': 919, 'tough': 920, 'whispers': 921, 'phil': 922, 'threw': 923, 'straight': 924, 'belles': 925, 'moor': 926, 'brand': 927, 'shapes': 928, 'work': 929, 'vow': 930, 'blarney': 931, 'paid': 932, 'bower': 933, 'remain': 934, 'charming': 935, 'storied': 936, 'chieftains': 937, 'slaughter': 938, 'bann': 939, 'boyne': 940, 'liffey': 941, 'gallant': 942, 'awake': 943, 'greet': 944, 'meadow': 945, 'sweeter': 946, 'dirty': 947, 'cats': 948, 'crossed': 949, 'field': 950, 'river': 951, 'full': 952, 'aroon': 953, 'sends': 954, 'woe': 955, 'chain': 956, 'main': 957, 'charms': 958, 'fondly': 959, 'fleet': 960, 'fairy': 961, 'thine': 962, 'known': 963, 'truly': 964, 'close': 965, 'story': 966, 'flag': 967, 'sweetest': 968, 'honor': 969, 'playing': 970, 'mauser': 971, 'music': 972, 'tom': 973, 'hurrah': 974, 'big': 975, 'lead': 976, 'south': 977, 'generation': 978, 'freedom': 979, 'agin': 980, 'creature': 981, 'dad': 982, 'venture': 983, 'word': 984, 'wonderful': 985, 'crazy': 986, 'lazy': 987, 'grave': 988, 'jest': 989, 'remark': 990, 'strangers': 991, 'strong': 992, 'shook': 993, 'walk': 994, 'north': 995, 'ours': 996, 'cease': 997, 'strife': 998, 'whats': 999, 'lilacs': 1000, 'prove': 1001, 'sweetheart': 1002, 'letters': 1003, 'sent': 1004, 'speak': 1005, 'brow': 1006, 'albert': 1007, 'mooney': 1008, 'fighting': 1009, 'fingers': 1010, 'toes': 1011, 'john': 1012, 'hurroo': 1013, 'drums': 1014, 'beguiled': 1015, 'carry': 1016, 'bone': 1017, 'havent': 1018, 'walkin': 1019, 'kilgary': 1020, 'pepper': 1021, 'countin': 1022, 'forth': 1023, 'deliver': 1024, 'daddy': 1025, 'em': 1026, 'deceive': 1027, 'between': 1028, 'even': 1029, 'prisoner': 1030, 'fists': 1031, 'knocked': 1032, 'carriages': 1033, 'rollin': 1034, 'juice': 1035, 'courtin': 1036, 'ponchartrain': 1037, 'does': 1038, 'stranger': 1039, 'marry': 1040, 'adieu': 1041, 'ask': 1042, 'tipped': 1043, 'arrived': 1044, 'ladies': 1045, 'potatoes': 1046, 'courting': 1047, 'miss': 1048, 'small': 1049, 'ned': 1050, 'ribbons': 1051, 'heel': 1052, 'bonny': 1053, 'pipe': 1054, 'thrush': 1055, 'sweethearts': 1056, 'unto': 1057, 'rise': 1058, 'softly': 1059, 'milking': 1060, 'rare': 1061, 'pity': 1062, 'treasure': 1063, 'noon': 1064, 'sailing': 1065, 'banish': 1066, 'riches': 1067, 'comfort': 1068, 'yonder': 1069, 'flows': 1070, 'fairer': 1071, 'lass': 1072, 'woods': 1073, 'strayed': 1074, 'locks': 1075, 'breaking': 1076, 'june': 1077, 'started': 1078, 'hearted': 1079, 'beer': 1080, 'daylight': 1081, 'among': 1082, 'bundle': 1083, 'connaught': 1084, 'quay': 1085, 'erins': 1086, 'galway': 1087, 'fearless': 1088, 'bravely': 1089, 'marches': 1090, 'fate': 1091, 'neck': 1092, 'trod': 1093, 'marched': 1094, 'antrim': 1095, 'sash': 1096, 'flashed': 1097, 'hath': 1098, 'foemans': 1099, 'fight': 1100, 'heavy': 1101, 'bore': 1102, 'mans': 1103, 'counter': 1104, 'dozen': 1105, 'gallon': 1106, 'bottles': 1107, 'diamond': 1108, 'resemble': 1109, 'tiny': 1110, 'friendly': 1111, 'weather': 1112, 'inside': 1113, 'remember': 1114, 'someone': 1115, 'hat': 1116, 'body': 1117, 'dancers': 1118, 'hanging': 1119, 'empty': 1120, 'shoes': 1121, 'broke': 1122, 'december': 1123, 'move': 1124, 'reason': 1125, 'roof': 1126, 'naught': 1127, 'tower': 1128, 'power': 1129, 'king': 1130, 'dreaming': 1131, 'crew': 1132, 'whos': 1133, 'mccann': 1134, 'smoke': 1135, 'notes': 1136, 'yeoman': 1137, 'cavalry': 1138, 'guard': 1139, 'forced': 1140, 'brother': 1141, 'cousin': 1142, 'blame': 1143, 'croppy': 1144, 'dressed': 1145, 'trees': 1146, 'wore': 1147, 'words': 1148, 'swiftly': 1149, 'dawn': 1150, 'lovd': 1151, 'voices': 1152, 'moaning': 1153, 'dark': 1154, 'gather': 1155, 'tay': 1156, 'swinging': 1157, 'drinkin': 1158, 'sitting': 1159, 'stile': 1160, 'springing': 1161, 'yours': 1162, 'kept': 1163, 'aisey': 1164, 'rub': 1165, 'dub': 1166, 'dow': 1167, 'shelah': 1168, 'fairly': 1169, 'beggarman': 1170, 'begging': 1171, 'slept': 1172, 'holes': 1173, 'coming': 1174, 'thru': 1175, 'boo': 1176, 'lady': 1177, 'kerry': 1178, 'pipers': 1179, 'laugh': 1180, 'beaming': 1181, 'guineas': 1182, 'least': 1183, 'diggin': 1184, 'mourne': 1185, 'spending': 1186, 'mellow': 1187, 'plying': 1188, 'slowly': 1189, 'mooncoin': 1190, 'flow': 1191, 'sounds': 1192, 'shine': 1193, 'cool': 1194, 'crystal': 1195, 'fountain': 1196, 'moonlight': 1197, 'grandmother': 1198, 'crooning': 1199, 'merrily': 1200, 'spins': 1201, 'lightly': 1202, 'moving': 1203, 'lattice': 1204, 'grove': 1205, 'swings': 1206, 'finger': 1207, 'shamrock': 1208, 'pocket': 1209, 'springtime': 1210, 'gilgarra': 1211, 'rapier': 1212, 'ringum': 1213, 'mornin': 1214, 'heather': 1215, 'build': 1216, 'maidens': 1217, 'prime': 1218, 'nlyme': 1219, 'flavours': 1220, 'lusty': 1221, 'reminded': 1222, 'attend': 1223, 'guardian': 1224, 'creeping': 1225, 'dale': 1226, 'vigil': 1227, 'visions': 1228, 'revealing': 1229, 'breathes': 1230, 'holy': 1231, 'strains': 1232, 'hover': 1233, 'hark': 1234, 'solemn': 1235, 'winging': 1236, 'earthly': 1237, 'shalt': 1238, 'awaken': 1239, 'destiny': 1240, 'emigrants': 1241, 'amid': 1242, 'longing': 1243, 'parted': 1244, 'townland': 1245, 'vessel': 1246, 'crowded': 1247, 'disquieted': 1248, 'folk': 1249, 'escape': 1250, 'hardship': 1251, 'sustaining': 1252, 'glimpse': 1253, 'faded': 1254, 'strangely': 1255, 'seas': 1256, 'anger': 1257, 'desperate': 1258, 'plight': 1259, 'worsened': 1260, 'delirium': 1261, 'possessed': 1262, 'clouded': 1263, 'prayers': 1264, 'begged': 1265, 'forgiveness': 1266, 'seeking': 1267, 'distant': 1268, 'mither': 1269, 'simple': 1270, 'ditty': 1271, 'ld': 1272, 'li': 1273, 'hush': 1274, 'lullaby': 1275, 'huggin': 1276, 'hummin': 1277, 'rock': 1278, 'asleep': 1279, 'outside': 1280, 'modestly': 1281, 'ry': 1282, 'ay': 1283, 'di': 1284, 're': 1285, 'dai': 1286, 'rie': 1287, 'shc': 1288, 'bridle': 1289, 'stable': 1290, 'oats': 1291, 'eat': 1292, 'soldier': 1293, 'aisy': 1294, 'arose': 1295, 'christmas': 1296, '1803': 1297, 'australia': 1298, 'marks': 1299, 'carried': 1300, 'rusty': 1301, 'iron': 1302, 'wains': 1303, 'mainsails': 1304, 'unfurled': 1305, 'curses': 1306, 'hurled': 1307, 'swell': 1308, 'moth': 1309, 'firelights': 1310, 'horses': 1311, 'rode': 1312, 'taking': 1313, 'hades': 1314, 'twilight': 1315, 'forty': 1316, 'slime': 1317, 'climate': 1318, 'bravery': 1319, 'ended': 1320, 'bond': 1321, 'rebel': 1322, 'iii': 1323, 'violin': 1324, 'clay': 1325, 'sooner': 1326, 'sport': 1327, 'colour': 1328, 'knows': 1329, 'earth': 1330, 'serve': 1331, 'clyde': 1332, 'mourn': 1333, 'weep': 1334, 'suffer': 1335, 'diamonds': 1336, 'queen': 1337, 'hung': 1338, 'tied': 1339, 'apprenticed': 1340, 'happiness': 1341, 'misfortune': 1342, 'follow': 1343, 'strolling': 1344, 'selling': 1345, 'bar': 1346, 'customer': 1347, 'slipped': 1348, 'luck': 1349, 'jury': 1350, 'trial': 1351, 'case': 1352, 'warning': 1353, 'liquor': 1354, 'porter': 1355, 'pleasures': 1356, 'fishing': 1357, 'farming': 1358, 'glens': 1359, 'softest': 1360, 'dripping': 1361, 'snare': 1362, 'lose': 1363, 'court': 1364, 'primrose': 1365, 'bee': 1366, 'hopeless': 1367, 'wonders': 1368, 'admiration': 1369, 'haunt': 1370, 'wherever': 1371, 'sands': 1372, 'purer': 1373, 'within': 1374, 'grieve': 1375, 'drumslieve': 1376, 'ballygrant': 1377, 'deepest': 1378, 'boatsman': 1379, 'ferry': 1380, 'childhood': 1381, 'reflections': 1382, 'boyhood': 1383, 'melting': 1384, 'roaming': 1385, 'reported': 1386, 'marble': 1387, 'stones': 1388, 'ink': 1389, 'support': 1390, 'drunk': 1391, 'seldom': 1392, 'sick': 1393, 'numbered': 1394, 'foam': 1395, 'compare': 1396, 'sights': 1397, 'coast': 1398, 'clare': 1399, 'kilkee': 1400, 'kilrush': 1401, 'watching': 1402, 'pheasants': 1403, 'homes': 1404, 'streams': 1405, 'dublins': 1406, 'cockles': 1407, 'mussels': 1408, 'fish': 1409, 'monger': 1410, 'ghost': 1411, 'wheels': 1412, 'eden': 1413, 'vanished': 1414, 'finea': 1415, 'halfway': 1416, 'cootehill': 1417, 'gruff': 1418, 'whispering': 1419, 'crow': 1420, 'newborn': 1421, 'babies': 1422, 'huff': 1423, 'start': 1424, 'sorrowful': 1425, 'squall': 1426, 'babys': 1427, 'toil': 1428, 'worn': 1429, 'fore': 1430, 'flute': 1431, 'yer': 1432, 'boot': 1433, 'magee': 1434, 'scruff': 1435, 'slanderin': 1436, 'marchin': 1437, 'assisted': 1438, 'drain': 1439, 'dudeen': 1440, 'puff': 1441, 'whisperings': 1442, 'barrin': 1443, 'chocolate': 1444, 'feegee': 1445, 'sort': 1446, 'moonshiny': 1447, 'stuff': 1448, 'addle': 1449, 'brain': 1450, 'ringin': 1451, 'glamour': 1452, 'gas': 1453, 'guff': 1454, 'whisper': 1455, 'oil': 1456, 'remarkable': 1457, 'policeman': 1458, 'bluff': 1459, 'maintain': 1460, 'guril': 1461, 'sic': 1462, 'passage': 1463, 'rough': 1464, 'borne': 1465, 'breeze': 1466, 'boundless': 1467, 'stupendous': 1468, 'roll': 1469, 'thundering': 1470, 'motion': 1471, 'mermaids': 1472, 'fierce': 1473, 'tempest': 1474, 'gathers': 1475, 'oneill': 1476, 'odonnell': 1477, 'lucan': 1478, 'oconnell': 1479, 'brian': 1480, 'drove': 1481, 'danes': 1482, 'patrick': 1483, 'vermin': 1484, 'whose': 1485, 'benburb': 1486, 'blackwater': 1487, 'owen': 1488, 'roe': 1489, 'munroe': 1490, 'lambs': 1491, 'skip': 1492, 'views': 1493, 'enchanting': 1494, 'rostrevor': 1495, 'groves': 1496, 'lakes': 1497, 'ride': 1498, 'tide': 1499, 'majestic': 1500, 'shannon': 1501, 'sail': 1502, 'loch': 1503, 'neagh': 1504, 'ross': 1505, 'gorey': 1506, 'saxon': 1507, 'tory': 1508, 'soil': 1509, 'sanctified': 1510, 'enemies': 1511, 'links': 1512, 'encumbered': 1513, 'resound': 1514, 'hosannahs': 1515, 'bide': 1516, 'hushed': 1517, 'lying': 1518, 'kneel': 1519, 'ave': 1520, 'tread': 1521, 'fail': 1522, 'simply': 1523, 'gasworks': 1524, 'croft': 1525, 'dreamed': 1526, 'canal': 1527, 'factory': 1528, 'clouds': 1529, 'drifting': 1530, 'prowling': 1531, 'beat': 1532, 'springs': 1533, 'siren': 1534, 'docks': 1535, 'train': 1536, 'smelled': 1537, 'smokey': 1538, 'sharp': 1539, 'axe': 1540, 'steel': 1541, 'tempered': 1542, 'chop': 1543, 't': 1544, 'agree': 1545, 'leaning': 1546, 'weirs': 1547, 'ray': 1548, 'glow': 1549, 'changeless': 1550, 'constant': 1551, 'bounding': 1552, 'castles': 1553, 'sacked': 1554, 'scattered': 1555, 'fixed': 1556, 'endearing': 1557, 'gifts': 1558, 'fading': 1559, 'wouldst': 1560, 'adored': 1561, 'loveliness': 1562, 'ruin': 1563, 'itself': 1564, 'verdantly': 1565, 'unprofaned': 1566, 'fervor': 1567, 'faith': 1568, 'forgets': 1569, 'sunflower': 1570, 'rag': 1571, 'games': 1572, 'hold': 1573, 'defend': 1574, 'veteran': 1575, 'volunteers': 1576, 'pat': 1577, 'pearse': 1578, 'clark': 1579, 'macdonagh': 1580, 'macdiarmada': 1581, 'mcbryde': 1582, 'james': 1583, 'connolly': 1584, 'placed': 1585, 'machine': 1586, 'ranting': 1587, 'hour': 1588, 'bullet': 1589, 'stuck': 1590, 'craw': 1591, 'poisoning': 1592, 'ceannt': 1593, 'lions': 1594, 'union': 1595, 'poured': 1596, 'dismay': 1597, 'horror': 1598, 'englishmen': 1599, 'khaki': 1600, 'renown': 1601, 'fame': 1602, 'forefathers': 1603, 'blaze': 1604, 'priests': 1605, 'offer': 1606, 'charmin': 1607, 'variety': 1608, 'renownd': 1609, 'learnin': 1610, 'piety': 1611, 'advance': 1612, 'widout': 1613, 'impropriety': 1614, 'flowr': 1615, 'cho': 1616, 'powrfulest': 1617, 'preacher': 1618, 'tenderest': 1619, 'teacher': 1620, 'kindliest': 1621, 'donegal': 1622, 'talk': 1623, 'provost': 1624, 'trinity': 1625, 'famous': 1626, 'greek': 1627, 'latinity': 1628, 'divils': 1629, 'divinity': 1630, 'd': 1631, 'likes': 1632, 'logic': 1633, 'mythology': 1634, 'thayology': 1635, 'conchology': 1636, 'sinners': 1637, 'wishful': 1638, 'childer': 1639, 'avick': 1640, 'gad': 1641, 'flock': 1642, 'grandest': 1643, 'control': 1644, 'checking': 1645, 'coaxin': 1646, 'onaisy': 1647, 'lifting': 1648, 'avoidin': 1649, 'frivolity': 1650, 'seasons': 1651, 'innocent': 1652, 'jollity': 1653, 'playboy': 1654, 'claim': 1655, 'equality': 1656, 'comicality': 1657, 'bishop': 1658, 'lave': 1659, 'gaiety': 1660, 'laity': 1661, 'clergy': 1662, 'jewels': 1663, 'plundering': 1664, 'pillage': 1665, 'starved': 1666, 'cries': 1667, 'thems': 1668, 'bondage': 1669, 'fourth': 1670, 'tabhair': 1671, 'dom': 1672, 'lmh': 1673, 'harmony': 1674, 'east': 1675, 'destroy': 1676, 'command': 1677, 'gesture': 1678, 'troubles': 1679, 'weak': 1680, 'peoples': 1681, 'creeds': 1682, 'lets': 1683, 'needs': 1684, 'passion': 1685, 'fashion': 1686, 'guide': 1687, 'share': 1688, 'sparkling': 1689, 'meeting': 1690, 'iull': 1691, 'contented': 1692, 'ache': 1693, 'painful': 1694, 'wrote': 1695, 'twisted': 1696, 'twined': 1697, 'cheek': 1698, 'bedim': 1699, 'holds': 1700, 'smiles': 1701, 'scarcely': 1702, 'darkning': 1703, 'beyond': 1704, 'yearn': 1705, 'laughs': 1706, 'humble': 1707, 'brightest': 1708, 'gleam': 1709, 'forgot': 1710, 'pulled': 1711, 'comb': 1712, 'counting': 1713, 'knock': 1714, 'murray': 1715, 'fellow': 1716, 'hail': 1717, 'tumblin': 1718, 'apple': 1719, 'pie': 1720, 'gets': 1721, 'doleful': 1722, 'enemy': 1723, 'nearly': 1724, 'slew': 1725, 'queer': 1726, 'mild': 1727, 'legs': 1728, 'indeed': 1729, 'island': 1730, 'sulloon': 1731, 'flesh': 1732, 'yere': 1733, 'armless': 1734, 'boneless': 1735, 'chickenless': 1736, 'egg': 1737, 'yell': 1738, 'bowl': 1739, 'rolling': 1740, 'swearing': 1741, 'rattled': 1742, 'saber': 1743, 'deceiver': 1744, 'rig': 1745, 'um': 1746, 'du': 1747, 'rum': 1748, 'jar': 1749, 'shinin': 1750, 'coins': 1751, 'promised': 1752, 'vowed': 1753, 'devils': 1754, 'awakened': 1755, 'six': 1756, 'guards': 1757, 'numbers': 1758, 'odd': 1759, 'flew': 1760, 'mistaken': 1761, 'mollys': 1762, 'robbing': 1763, 'sentry': 1764, 'sligo': 1765, 'fishin': 1766, 'bowlin': 1767, 'others': 1768, 'railroad': 1769, 'ties': 1770, 'crossings': 1771, 'swamps': 1772, 'elevations': 1773, 'resolved': 1774, 'sunset': 1775, 'higher': 1776, 'win': 1777, 'allegators': 1778, 'wood': 1779, 'treated': 1780, 'shoulders': 1781, 'paint': 1782, 'picture': 1783, 'vain': 1784, 'returned': 1785, 'cottage': 1786, 'sociable': 1787, 'foaming': 1788, 'n': 1789, 'jeremy': 1790, 'lanigan': 1791, 'battered': 1792, 'hadnt': 1793, 'pound': 1794, 'farm': 1795, 'acres': 1796, 'party': 1797, 'listen': 1798, 'glisten': 1799, 'rows': 1800, 'ructions': 1801, 'invitation': 1802, 'minute': 1803, 'bees': 1804, 'cask': 1805, 'judy': 1806, 'odaly': 1807, 'milliner': 1808, 'wink': 1809, 'peggy': 1810, 'mcgilligan': 1811, 'lashings': 1812, 'punch': 1813, 'cakes': 1814, 'bacon': 1815, 'tea': 1816, 'nolans': 1817, 'dolans': 1818, 'ogradys': 1819, 'sounded': 1820, 'taras': 1821, 'hall': 1822, 'nelly': 1823, 'gray': 1824, 'rat': 1825, 'catchers': 1826, 'doing': 1827, 'kinds': 1828, 'nonsensical': 1829, 'polkas': 1830, 'whirligig': 1831, 'julia': 1832, 'banished': 1833, 'nonsense': 1834, 'twist': 1835, 'jig': 1836, 'mavrone': 1837, 'mad': 1838, 'ceiling': 1839, 'brooks': 1840, 'academy': 1841, 'learning': 1842, 'learn': 1843, 'couples': 1844, 'groups': 1845, 'accident': 1846, 'happened': 1847, 'terrance': 1848, 'mccarthy': 1849, 'finnertys': 1850, 'hoops': 1851, 'cried': 1852, 'meelia': 1853, 'murther': 1854, 'gathered': 1855, 'carmody': 1856, 'further': 1857, 'satisfaction': 1858, 'midst': 1859, 'kerrigan': 1860, 'declared': 1861, 'painted': 1862, 'suppose': 1863, 'morgan': 1864, 'powerful': 1865, 'stretched': 1866, 'smashed': 1867, 'chaneys': 1868, 'runctions': 1869, 'lick': 1870, 'phelim': 1871, 'mchugh': 1872, 'replied': 1873, 'introduction': 1874, 'kicked': 1875, 'terrible': 1876, 'hullabaloo': 1877, 'piper': 1878, 'strangled': 1879, 'squeezed': 1880, 'bellows': 1881, 'chanters': 1882, 'entangled': 1883, 'gaily': 1884, 'mairis': 1885, 'hillways': 1886, 'myrtle': 1887, 'bracken': 1888, 'sheilings': 1889, 'sake': 1890, 'rowans': 1891, 'herring': 1892, 'meal': 1893, 'peat': 1894, 'creel': 1895, 'bairns': 1896, 'weel': 1897, 'toast': 1898, 'soar': 1899, 'blackbird': 1900, 'note': 1901, 'linnet': 1902, 'lure': 1903, 'cozy': 1904, 'catch': 1905, 'company': 1906, 'harm': 1907, 'wit': 1908, 'recall': 1909, 'leisure': 1910, 'awhile': 1911, 'sorely': 1912, 'ruby': 1913, 'enthralled': 1914, 'sorry': 1915, 'theyd': 1916, 'falls': 1917, 'lot': 1918, 'tuned': 1919, 'bough': 1920, 'cow': 1921, 'chanting': 1922, 'melodious': 1923, 'scarce': 1924, 'soothed': 1925, 'solace': 1926, 'courtesy': 1927, 'salute': 1928, 'amiable': 1929, 'captive': 1930, 'slave': 1931, 'future': 1932, 'banter': 1933, 'enamour': 1934, 'indies': 1935, 'afford': 1936, 'transparently': 1937, 'flame': 1938, 'add': 1939, 'fuel': 1940, 'grant': 1941, 'desire': 1942, 'expire': 1943, 'wealth': 1944, 'damer': 1945, 'african': 1946, 'devonshire': 1947, 'lamp': 1948, 'alladin': 1949, 'genie': 1950, 'also': 1951, 'withdraw': 1952, 'tease': 1953, 'single': 1954, 'airy': 1955, 'embarrass': 1956, 'besides': 1957, 'almanack': 1958, 'useless': 1959, 'date': 1960, 'ware': 1961, 'rate': 1962, 'fragrance': 1963, 'loses': 1964, 'consumed': 1965, 'october': 1966, 'knowing': 1967, 'steer': 1968, 'blast': 1969, 'danger': 1970, 'farthing': 1971, 'affection': 1972, 'enjoy': 1973, 'choose': 1974, 'killarneys': 1975, 'sister': 1976, 'pains': 1977, 'loss': 1978, 'tuam': 1979, 'saluted': 1980, 'drank': 1981, 'pint': 1982, 'smother': 1983, 'reap': 1984, 'cut': 1985, 'goblins': 1986, 'bought': 1987, 'brogues': 1988, 'rattling': 1989, 'bogs': 1990, 'frightning': 1991, 'dogs': 1992, 'hunt': 1993, 'hare': 1994, 'follol': 1995, 'rah': 1996, 'mullingar': 1997, 'rested': 1998, 'limbs': 1999, 'blithe': 2000, 'heartfrom': 2001, 'paddys': 2002, 'cure': 2003, 'lassies': 2004, 'laughing': 2005, 'curious': 2006, 'style': 2007, 'twould': 2008, 'bubblin': 2009, 'hired': 2010, 'wages': 2011, 'required': 2012, 'almost': 2013, 'deprived': 2014, 'stroll': 2015, 'quality': 2016, 'locality': 2017, 'something': 2018, 'wobblin': 2019, 'enquiring': 2020, 'rogue': 2021, 'brogue': 2022, 'wasnt': 2023, 'vogue': 2024, 'spirits': 2025, 'falling': 2026, 'jumped': 2027, 'aboard': 2028, 'pigs': 2029, 'rigs': 2030, 'jigs': 2031, 'bubbling': 2032, 'holyhead': 2033, 'wished': 2034, 'instead': 2035, 'bouys': 2036, 'liverpool': 2037, 'safely': 2038, 'fool': 2039, 'boil': 2040, 'temper': 2041, 'losing': 2042, 'abusing': 2043, 'shillelagh': 2044, 'nigh': 2045, 'hobble': 2046, 'load': 2047, 'hurray': 2048, 'joined': 2049, 'affray': 2050, 'quitely': 2051, 'cleared': 2052, 'host': 2053, 'march': 2054, 'faces': 2055, 'farmstead': 2056, 'fishers': 2057, 'ban': 2058, 'vengeance': 2059, 'hapless': 2060, 'about': 2061, 'hemp': 2062, 'rope': 2063, 'clung': 2064, 'grim': 2065, 'array': 2066, 'earnest': 2067, 'stalwart': 2068, 'stainless': 2069, 'banner': 2070, 'marching': 2071, 'torn': 2072, 'furious': 2073, 'odds': 2074, 'keen': 2075, 'toomebridge': 2076, 'treads': 2077, 'upwards': 2078, 'traveled': 2079, 'quarters': 2080, 'below': 2081, 'hogshead': 2082, 'stack': 2083, 'stagger': 2084, 'dig': 2085, 'hole': 2086, 'couple': 2087, 'scratch': 2088, 'consolation': 2089, 'tyrant': 2090, 'remorseless': 2091, 'foe': 2092, 'lift': 2093, 'stranded': 2094, 'prince': 2095, 'edward': 2096, 'coffee': 2097, 'trace': 2098, 'fiddlin': 2099, 'dime': 2100, 'shy': 2101, 'hello': 2102, 'wintry': 2103, 'yellow': 2104, 'somewhere': 2105, 'written': 2106, 'begin': 2107, 'tap': 2108, 'caught': 2109, 'leap': 2110, 'clumsy': 2111, 'graceful': 2112, 'fiddlers': 2113, 'everywhere': 2114, 'boots': 2115, 'laughtcr': 2116, 'suits': 2117, 'easter': 2118, 'gowns': 2119, 'sailors': 2120, 'pianos': 2121, 'setting': 2122, 'someones': 2123, 'hats': 2124, 'rack': 2125, 'chair': 2126, 'wooden': 2127, 'feels': 2128, 'touch': 2129, 'awaitin': 2130, 'thc': 2131, 'fiddles': 2132, 'closet': 2133, 'strings': 2134, 'tbe': 2135, 'covers': 2136, 'buttoned': 2137, 'sometimes': 2138, 'melody': 2139, 'passes': 2140, 'slight': 2141, 'lack': 2142, 'moved': 2143, 'homeward': 2144, 'swan': 2145, 'moves': 2146, 'goods': 2147, 'gear': 2148, 'din': 2149, 'rude': 2150, 'wherein': 2151, 'dwell': 2152, 'abandon': 2153, 'energy': 2154, 'blight': 2155, 'praties': 2156, 'sheep': 2157, 'cattle': 2158, 'taxes': 2159, 'unpaid': 2160, 'redeem': 2161, 'bleak': 2162, 'landlord': 2163, 'sheriff': 2164, 'spleen': 2165, 'heaved': 2166, 'sigh': 2167, 'bade': 2168, 'goodbye': 2169, 'stony': 2170, 'anguish': 2171, 'seeing': 2172, 'feeble': 2173, 'frame': 2174, 'wrapped': 2175, 'cta': 2176, 'mr': 2177, 'unseen': 2178, 'stern': 2179, 'rally': 2180, 'cheer': 2181, 'revenge': 2182, 'waking': 2183, 'wisdom': 2184, 'dwelling': 2185, 'battleshield': 2186, 'dignity': 2187, 'shelter': 2188, 'heed': 2189, 'inheritance': 2190, 'heavem': 2191, 'heaven': 2192, 'victory': 2193, 'reach': 2194, 'whatever': 2195, 'befall': 2196, 'ruler': 2197, 'pleasant': 2198, 'rambling': 2199, 'board': 2200, 'followed': 2201, 'shortly': 2202, 'anchor': 2203, '23rd': 2204, 'lrelands': 2205, 'daughters': 2206, 'crowds': 2207, 'assembled': 2208, 'fulfill': 2209, 'jovial': 2210, 'conversations': 2211, 'neighbors': 2212, 'turning': 2213, 'tailor': 2214, 'quigley': 2215, 'bould': 2216, 'britches': 2217, 'lived': 2218, 'flying': 2219, 'dove': 2220, 'hiii': 2221, 'dreamt': 2222, 'joking': 2223, 'manys': 2224, 'cock': 2225, 'shrill': 2226, 'awoke': 2227, 'california': 2228, 'miles': 2229, 'banbridge': 2230, 'july': 2231, 'boreen': 2232, 'sheen': 2233, 'coaxing': 2234, 'elf': 2235, 'shake': 2236, 'bantry': 2237, 'onward': 2238, 'sped': 2239, 'gazed': 2240, 'passerby': 2241, 'gem': 2242, 'irelands': 2243, 'travelled': 2244, 'hit': 2245, 'career': 2246, 'square': 2247, 'surrendered': 2248, 'tenant': 2249, 'shawl': 2250, 'gown': 2251, 'crossroads': 2252, 'dress': 2253, 'try': 2254, 'sheeps': 2255, 'deludhering': 2256, 'yoke': 2257, 'rust': 2258, 'plow': 2259, 'fireside': 2260, 'sits': 2261, 'whistle': 2262, 'changing': 2263, 'fright': 2264, 'downfall': 2265, 'cornwall': 2266, 'parlour': 2267, 'passing': 2268, 'william': 2269, 'betray': 2270, 'guinea': 2271, 'walking': 2272, 'mounted': 2273, 'platform': 2274, 'deny': 2275, 'walked': 2276, 'margin': 2277, 'lough': 2278, 'leane': 2279, 'bloomed': 2280, 'whom': 2281, 'cap': 2282, 'cloak': 2283, 'glossy': 2284, 'pail': 2285, 'palm': 2286, 'venus': 2287, 'bank': 2288, 'travelians': 2289, 'babes': 2290, 'freebirds': 2291, 'grew': 2292, 'matters': 2293, 'famine': 2294, 'rebelled': 2295, 'windswept': 2296, 'harbour': 2297, 'botany': 2298, 'whilst': 2299, 'wan': 2300, 'cloud': 2301, 'shannons': 2302, 'returnd': 2303, 'doubts': 2304, 'fears': 2305, 'aching': 2306, 'seemd': 2307, 'mingling': 2308, 'flood': 2309, 'path': 2310, 'wrath': 2311, 'lamenting': 2312, 'sudden': 2313, 'kissd': 2314, 'showrs': 2315, 'flowing': 2316, 'laughd': 2317, 'beam': 2318, 'soared': 2319, 'aloft': 2320, 'phantom': 2321, 'outspread': 2322, 'throbbing': 2323, 'hid': 2324, 'treasures': 2325, 'pots': 2326, 'tin': 2327, 'cans': 2328, 'mash': 2329, 'bran': 2330, 'barney': 2331, 'peeled': 2332, 'searching': 2333, 'connemara': 2334, 'butcher': 2335, 'quart': 2336, 'bottle': 2337, 'help': 2338, 'gate': 2339, 'glory': 2340, 'lane': 2341, 'village': 2342, 'church': 2343, 'spire': 2344, 'graveyard': 2345, 'baby': 2346, 'blessing': 2347, 'hoping': 2348, 'trust': 2349, 'strength': 2350, 'thank': 2351, 'bidding': 2352, 'bread': 2353, 'shines': 2354, 'fifty': 2355, 'often': 2356, 'shut': 2357, 'frisky': 2358, 'pig': 2359, 'whisky': 2360, 'uncle': 2361, 'enlisted': 2362, 'trudged': 2363, 'bosom': 2364, 'daisy': 2365, 'drubbing': 2366, 'shirts': 2367, 'battle': 2368, 'blows': 2369, 'pate': 2370, 'bothered': 2371, 'rarely': 2372, 'dropped': 2373, 'honest': 2374, 'thinks': 2375, 'eight': 2376, 'score': 2377, 'basin': 2378, 'zoo': 2379, 'everybody': 2380, 'calls': 2381, 'trades': 2382, 'dinner': 2383, 'slip': 2384, 'corner': 2385, 'barn': 2386, 'currabawn': 2387, 'shocking': 2388, 'wet': 2389, 'raindrops': 2390, 'rats': 2391, 'peek': 2392, 'waken': 2393, 'spotted': 2394, 'apron': 2395, 'calico': 2396, 'blouse': 2397, 'frighten': 2398, 'afraid': 2399, 'flaxen': 2400, 'haired': 2401, 'rags': 2402, 'tags': 2403, 'leggins': 2404, 'collar': 2405, 'tie': 2406, 'goggles': 2407, 'fashioned': 2408, 'bag': 2409, 'bulging': 2410, 'sack': 2411, 'peeping': 2412, 'skin': 2413, 'rink': 2414, 'doodle': 2415, 'getting': 2416, 'raked': 2417, 'gladness': 2418, 'tuning': 2419, 'fills': 2420, 'eily': 2421, 'prouder': 2422, 'thady': 2423, 'boldly': 2424, 'lasses': 2425, 'fled': 2426, 'silent': 2427, 'glad': 2428, 'echo': 2429, 'companions': 2430, 'soars': 2431, 'enchanted': 2432, 'granted': 2433, 'adoration': 2434, 'gives': 2435, 'joyous': 2436, 'elation': 2437, 'covered': 2438, 'winter': 2439, 'riding': 2440, 'cherry': 2441, 'coal': 2442, 'falter': 2443, 'bowed': 2444, 'bonnet': 2445, 'courteous': 2446, 'looks': 2447, 'engaging': 2448, 'sell': 2449, 'purse': 2450, 'yearly': 2451, 'need': 2452, 'market': 2453, 'gain': 2454, 'dearly': 2455, 'tarry': 2456, 'although': 2457, 'parlay': 2458, 'ranks': 2459, 'girded': 2460, 'slung': 2461, 'warrior': 2462, 'bard': 2463, 'betrays': 2464, 'rights': 2465, 'faithful': 2466, 'chords': 2467, 'asunder': 2468, 'sully': 2469, 'bravry': 2470, 'londons': 2471, 'sight': 2472, 'workin': 2473, 'sow': 2474, 'wheat': 2475, 'gangs': 2476, 'sweep': 2477, 'expressed': 2478, 'london': 2479, 'top': 2480, 'dresses': 2481, 'bath': 2482, 'startin': 2483, 'fashions': 2484, 'mccree': 2485, 'nature': 2486, 'designed': 2487, 'complexions': 2488, 'cream': 2489, 'regard': 2490, 'sip': 2491, 'colors': 2492, 'wait': 2493, 'waitin': 2494, 'sweeps': 2495, 'beauing': 2496, 'belling': 2497, 'windows': 2498, 'cursing': 2499, 'faster': 2500, 'waiters': 2501, 'bailiffs': 2502, 'duns': 2503, 'bacchus': 2504, 'begotten': 2505, 'politicians': 2506, 'funds': 2507, 'dadda': 2508, 'living': 2509, 'drives': 2510, 'having': 2511, 'racking': 2512, 'tenants': 2513, 'stewards': 2514, 'teasing': 2515, 'raising': 2516, 'wishing': 2517, 'sunny': 2518, 'doves': 2519, 'coo': 2520, 'neath': 2521, 'sunbeam': 2522, 'robin': 2523, 'waters': 2524, 'larks': 2525, 'join': 2526, 'breaks': 2527, 'oftimes': 2528, 'lilies': 2529, 'declining': 2530, 'vale': 2531, 'shades': 2532, 'mantle': 2533, 'spreading': 2534, 'listening': 2535, 'shedding': 2536, 'beginning': 2537, 'spinning': 2538, 'blind': 2539, 'drowsily': 2540, 'knitting': 2541, 'cheerily': 2542, 'noiselessly': 2543, 'whirring': 2544, 'foots': 2545, 'stirring': 2546, 'sprightly': 2547, 'chara': 2548, 'tapping': 2549, 'ivy': 2550, 'flapping': 2551, 'somebody': 2552, 'sighing': 2553, 'autumn': 2554, 'noise': 2555, 'chirping': 2556, 'holly': 2557, 'shoving': 2558, 'wrong': 2559, 'coolin': 2560, 'casement': 2561, 'rove': 2562, 'moons': 2563, 'brightly': 2564, 'shakes': 2565, 'lays': 2566, 'longs': 2567, 'lingers': 2568, 'glance': 2569, 'puts': 2570, 'lazily': 2571, 'easily': 2572, 'lowly': 2573, 'reels': 2574, 'noiseless': 2575, 'leaps': 2576, 'ere': 2577, 'lovers': 2578, 'roved': 2579, 'verdant': 2580, 'braes': 2581, 'skreen': 2582, 'countrie': 2583, 'foreign': 2584, 'strand': 2585, 'dewy': 2586, 'climb': 2587, 'rob': 2588, 'boat': 2589, 'sails': 2590, 'loaded': 2591, 'sink': 2592, 'leaned': 2593, 'oak': 2594, 'trusty': 2595, 'false': 2596, 'reached': 2597, 'pricked': 2598, 'waxes': 2599, 'fades': 2600, 'wholl': 2601, 'cockle': 2602, 'gloom': 2603, 'news': 2604, 'forbid': 2605, 'patricks': 2606, 'napper': 2607, 'tandy': 2608, 'hows': 2609, 'distressful': 2610, 'englands': 2611, 'remind': 2612, 'pull': 2613, 'throw': 2614, 'sod': 2615, 'root': 2616, 'underfoot': 2617, 'laws': 2618, 'blades': 2619, 'growin': 2620, 'dare': 2621, 'show': 2622, 'caubeen': 2623, 'year': 2624, 'returning': 2625, 'store': 2626, 'ale': 2627, 'frequent': 2628, 'landlady': 2629, 'credit': 2630, 'custom': 2631, 'sovereigns': 2632, 'landladys': 2633, 'wines': 2634, 'confess': 2635, 'pardon': 2636, 'prodigal': 2637, 'caress': 2638, 'forgive': 2639, 'ofttimes': 2640, 'wondering': 2641, 'powr': 2642, 'beguile': 2643, 'teardrop': 2644, 'lilting': 2645, 'laughters': 2646, 'twinkle': 2647, 'lilt': 2648, 'seems': 2649, 'linnets': 2650, 'real': 2651, 'regret': 2652, 'throughout': 2653, 'youths': 2654, 'chance': 2655, 'spied': 2656, 'receiver': 2657, 'counted': 2658, 'penny': 2659, 'bu': 2660, 'rungum': 2661, 'chamber': 2662, 'course': 2663, 'charges': 2664, 'filled': 2665, 'ready': 2666, 'footmen': 2667, 'likewise': 2668, 'draw': 2669, 'pistol': 2670, 'couldnt': 2671, 'shoot': 2672, 'robbin': 2673, 'jailer': 2674, 'tight': 2675, 'fisted': 2676, 'army': 2677, 'stationed': 2678, 'cork': 2679, 'roamin': 2680, 'swear': 2681, 'treat': 2682, 'sportin': 2683, 'hurley': 2684, 'bollin': 2685, 'maids': 2686, 'summertime': 2687, 'pluck': 2688, 'yon': 2689}\n",
            "2690\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aLHcnYnSrvq"
      },
      "source": [
        "input_sequences = []\n",
        "\n",
        "for line in corpus:\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    \n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, \n",
        "                                         maxlen=max_sequence_len, \n",
        "                                         padding='pre'))\n",
        "\n",
        "# create predictors and labels\n",
        "xs = input_sequences[:, :-1]\n",
        "labels = input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV4Zf82lTVkf",
        "outputId": "fdec47a3-987d-484c-da27-a7b053cf6ba0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8\n",
            "1\n",
            "71\n",
            "6\n",
            "713\n",
            "39\n",
            "1790\n",
            "1791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPUOrLUuTW8i",
        "outputId": "bcc1880a-169f-4e20-a3d6-1f1909cd6297",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0   51   12   96 1217   48\n",
            "    2]\n",
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imU_BrtTTYlG",
        "outputId": "9f6efa1b-fd83-40df-8d8c-29fd1ed39110",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(xs[6])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CICkOFVRTanr",
        "outputId": "e76748f4-475a-4216-b7eb-09d9bcd88ad2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(ys[6])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. ... 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Noli5UiMTbov",
        "outputId": "b13aeb4a-333c-4a95-92d0-67234edbf931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokenizer.word_index)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 1, 'and': 2, 'i': 3, 'to': 4, 'a': 5, 'of': 6, 'my': 7, 'in': 8, 'me': 9, 'for': 10, 'you': 11, 'all': 12, 'was': 13, 'she': 14, 'that': 15, 'on': 16, 'with': 17, 'her': 18, 'but': 19, 'as': 20, 'when': 21, 'love': 22, 'is': 23, 'your': 24, 'it': 25, 'will': 26, 'from': 27, 'by': 28, 'they': 29, 'be': 30, 'are': 31, 'so': 32, 'he': 33, 'old': 34, 'no': 35, 'oh': 36, 'ill': 37, 'at': 38, 'one': 39, 'his': 40, 'there': 41, 'were': 42, 'heart': 43, 'down': 44, 'now': 45, 'we': 46, 'where': 47, 'young': 48, 'never': 49, 'go': 50, 'come': 51, 'then': 52, 'did': 53, 'not': 54, 'said': 55, 'away': 56, 'their': 57, 'sweet': 58, 'them': 59, 'green': 60, 'if': 61, 'take': 62, 'our': 63, 'like': 64, 'night': 65, 'day': 66, 'o': 67, 'out': 68, 'fair': 69, 'this': 70, 'town': 71, 'have': 72, 'can': 73, 'true': 74, 'its': 75, 'thou': 76, 'see': 77, 'dear': 78, 'more': 79, 'theres': 80, 'or': 81, 'had': 82, 'would': 83, 'over': 84, 'hear': 85, 'up': 86, 'ive': 87, 'through': 88, 'home': 89, 'again': 90, 'well': 91, 'oer': 92, 'land': 93, 'good': 94, 'im': 95, 'ye': 96, 'sea': 97, 'left': 98, 'still': 99, 'father': 100, 'long': 101, 'rose': 102, 'could': 103, 'morning': 104, 'wild': 105, 'who': 106, 'eyes': 107, 'came': 108, 'while': 109, 'too': 110, 'back': 111, 'little': 112, 'an': 113, 'took': 114, 'him': 115, 'bow': 116, 'first': 117, 'let': 118, 'man': 119, 'shall': 120, 'know': 121, 'get': 122, 'high': 123, 'gone': 124, 'say': 125, 'ever': 126, 'some': 127, 'mary': 128, 'hand': 129, 'till': 130, 'put': 131, 'own': 132, 'time': 133, 'heard': 134, 'dead': 135, 'may': 136, 'bright': 137, 'mountain': 138, 'early': 139, 'rosin': 140, 'gave': 141, 'thee': 142, 'only': 143, 'far': 144, 'maid': 145, 'must': 146, 'find': 147, 'girl': 148, 'sure': 149, 'round': 150, 'dublin': 151, 'once': 152, 'world': 153, 'delight': 154, 'last': 155, 'johnny': 156, 'seen': 157, 'has': 158, 'fine': 159, 'road': 160, 'mother': 161, 'tis': 162, 'what': 163, 'way': 164, 'moon': 165, 'soul': 166, 'neer': 167, 'id': 168, 'just': 169, 'thats': 170, 'days': 171, 'darling': 172, 'went': 173, 'white': 174, 'die': 175, 'than': 176, 'hair': 177, 'goes': 178, 'meet': 179, 'today': 180, 'do': 181, 'girls': 182, 'shes': 183, 'thyme': 184, 'thy': 185, 'sing': 186, 'pretty': 187, 'new': 188, 'poor': 189, 'into': 190, 'life': 191, 'irish': 192, 'give': 193, 'boy': 194, 'youre': 195, 'make': 196, 'passed': 197, 'lovely': 198, 'black': 199, 'youll': 200, 'died': 201, 'red': 202, 'smile': 203, 'keep': 204, 'loves': 205, 'free': 206, 'leave': 207, 'friends': 208, 'each': 209, 'saw': 210, 'behind': 211, 'song': 212, 'ra': 213, 'dont': 214, 'arms': 215, 'am': 216, 'sun': 217, 'saying': 218, 'made': 219, 'wish': 220, 'cold': 221, 'met': 222, 'before': 223, 'should': 224, 'rocky': 225, 'light': 226, 'wid': 227, 'boys': 228, 'best': 229, 'fields': 230, 'since': 231, 'ball': 232, 'water': 233, 'casey': 234, 'mind': 235, 'along': 236, 'loved': 237, 'place': 238, 'ireland': 239, 'next': 240, 'three': 241, 'many': 242, 'years': 243, 'door': 244, 'us': 245, 'drink': 246, 'got': 247, 'might': 248, 'live': 249, 'roses': 250, 'play': 251, 'soon': 252, 'ground': 253, 'times': 254, 'spent': 255, 'going': 256, 'tree': 257, 'barley': 258, 'grass': 259, 'kind': 260, 'twas': 261, 'bridge': 262, 'around': 263, 'blue': 264, 'tell': 265, 'row': 266, 'how': 267, 'money': 268, 'merry': 269, 'stepped': 270, 'corporal': 271, 'always': 272, 'though': 273, 'near': 274, 'taken': 275, 'ones': 276, 'daughter': 277, 'forever': 278, 'loo': 279, 'shining': 280, 'plenty': 281, 'hes': 282, 'ship': 283, 'banks': 284, 'think': 285, 'very': 286, 'stand': 287, 'heres': 288, 'snow': 289, 'mountains': 290, 'molly': 291, 'wheel': 292, 'street': 293, 'erin': 294, 'side': 295, 'feet': 296, 'star': 297, 'look': 298, 'brave': 299, 'woman': 300, 'sons': 301, 'two': 302, 'says': 303, 'asked': 304, 'lanigans': 305, 'singing': 306, 'men': 307, 'toome': 308, 'stole': 309, 'god': 310, 'hill': 311, 'lonely': 312, 'lover': 313, 'tears': 314, 'fathers': 315, 'low': 316, 'voice': 317, 'quite': 318, 'able': 319, 'nice': 320, 'laid': 321, 'comrades': 322, 'wind': 323, 'another': 324, 'sit': 325, 'face': 326, 'band': 327, 'call': 328, 'colleen': 329, 'until': 330, 'hills': 331, 'mine': 332, 'above': 333, 'upon': 334, 'eer': 335, 'youve': 336, 'fly': 337, 'been': 338, 'late': 339, 'alive': 340, 'ballyjamesduff': 341, 'looked': 342, 'great': 343, 'why': 344, 'every': 345, 'proud': 346, 'found': 347, 'bragh': 348, 'such': 349, 'birds': 350, 'wedding': 351, 'welcome': 352, 'dancing': 353, 'da': 354, 'fell': 355, 'thinking': 356, 'roddy': 357, 'mccorley': 358, 'smiling': 359, 'mallow': 360, 'blooming': 361, 'thought': 362, 'peace': 363, 'soft': 364, 'pure': 365, 'harp': 366, 'dream': 367, 'alas': 368, 'yet': 369, 'clear': 370, 'art': 371, 'off': 372, 'hope': 373, 'fought': 374, 'mothers': 375, 'shore': 376, 'ago': 377, 'fol': 378, 'de': 379, 'house': 380, 'married': 381, 'bound': 382, 'danced': 383, 'devil': 384, 'dawning': 385, 'makes': 386, 'same': 387, 'sat': 388, 'any': 389, 'glass': 390, 'gay': 391, 'relations': 392, 'evening': 393, 'watched': 394, 'right': 395, 'fellows': 396, 'whiskey': 397, 'bonnie': 398, 'grows': 399, 'women': 400, 'flowers': 401, 'beauty': 402, 'cannot': 403, 'handsome': 404, 'happy': 405, 'gold': 406, 'rover': 407, 'none': 408, 'doneen': 409, 'summers': 410, 'people': 411, 'set': 412, 'paddy': 413, 'morn': 414, 'most': 415, 'easy': 416, 'struck': 417, 'beautiful': 418, 'those': 419, 'golden': 420, 'run': 421, 'pipes': 422, 'glen': 423, 'dying': 424, 'here': 425, 'wall': 426, 'across': 427, 'fire': 428, 'eileen': 429, 'longer': 430, 'cheeks': 431, 'valley': 432, 'both': 433, 'dew': 434, 'care': 435, 'bride': 436, 'nothing': 437, 'wont': 438, 'theyre': 439, 'colonel': 440, 'maiden': 441, 'shed': 442, 'til': 443, 'brown': 444, 'breast': 445, 'corn': 446, 'sinking': 447, 'began': 448, 'name': 449, 'cruel': 450, 'sound': 451, 'spancil': 452, 'county': 453, 'lies': 454, 'color': 455, 'thing': 456, 'decay': 457, 'sleep': 458, 'hours': 459, 'loving': 460, 'weary': 461, 'ringing': 462, 'please': 463, 'forget': 464, 'lie': 465, 'ran': 466, 'tore': 467, 'country': 468, 'fear': 469, 'fortune': 470, 'kissed': 471, 'alone': 472, 'ould': 473, 'cry': 474, 'dreams': 475, 'used': 476, 'horse': 477, 'break': 478, 'bells': 479, 'didnt': 480, 'weeks': 481, 'without': 482, 'raw': 483, 'nor': 484, 'twenty': 485, 'tune': 486, 'hed': 487, 'roving': 488, 'leaves': 489, 'cant': 490, 'death': 491, 'ten': 492, 'prison': 493, 'judge': 494, 'against': 495, 'lads': 496, 'shell': 497, 'fill': 498, 'valleys': 499, 'other': 500, 'pale': 501, 'joy': 502, 'wide': 503, 'bring': 504, 'ah': 505, 'cliffs': 506, 'city': 507, 'end': 508, 'turn': 509, 'sky': 510, 'born': 511, 'knew': 512, 'smiled': 513, 'rosie': 514, 'comes': 515, 'sayin': 516, 'lord': 517, 'dungannon': 518, 'blood': 519, 'air': 520, 'danny': 521, 'calling': 522, 'sunshine': 523, 'spring': 524, 'bid': 525, 'grow': 526, 'truth': 527, 'tear': 528, 'rings': 529, 'guns': 530, 'bay': 531, 'oflynn': 532, 'och': 533, 'stick': 534, 'rest': 535, 'four': 536, 'jewel': 537, 'tried': 538, 'grief': 539, 'answer': 540, 'kathleen': 541, 'fond': 542, 'eye': 543, 'goin': 544, 'pistols': 545, 'musha': 546, 'whack': 547, 'creole': 548, 'together': 549, 'room': 550, 'fall': 551, 'swore': 552, 'being': 553, 'step': 554, 'lark': 555, 'cailn': 556, 'deas': 557, 'crite': 558, 'na': 559, 'mb': 560, 'sir': 561, 'isle': 562, 'waiting': 563, 'magic': 564, 'skibbereen': 565, 'loud': 566, 'raise': 567, 'bent': 568, 'aged': 569, 'summer': 570, 'jenny': 571, 'excise': 572, 'rigadoo': 573, 'auld': 574, 'hearts': 575, 'nay': 576, 'stool': 577, 'farrell': 578, 'garden': 579, 'precious': 580, 'child': 581, 'slumber': 582, 'sleeping': 583, 'watch': 584, 'gently': 585, 'minstrel': 586, 'praise': 587, 'bell': 588, 'shaken': 589, 'immortal': 590, 'pray': 591, 'stay': 592, 'spoke': 593, 'cross': 594, 'brothers': 595, 'much': 596, 'past': 597, 'killarney': 598, 'sang': 599, 'tones': 600, 'ral': 601, 'wander': 602, 'cot': 603, 'feel': 604, 'yore': 605, 'answered': 606, 'divil': 607, 'middle': 608, 'bit': 609, 'led': 610, 'soldiers': 611, 'lily': 612, 'bed': 613, 'lassie': 614, 'clothes': 615, 'return': 616, 'broken': 617, 'derry': 618, 'sighed': 619, 'english': 620, 'tomorrow': 621, 'souls': 622, 'van': 623, 'diemans': 624, 'law': 625, 'neither': 626, 'winds': 627, 'rather': 628, 'doesnt': 629, 'rosy': 630, 'neatest': 631, 'hands': 632, 'whereon': 633, 'stands': 634, 'write': 635, 'thousand': 636, 'fare': 637, 'youd': 638, 'velvet': 639, 'neat': 640, 'landed': 641, 'health': 642, 'kellswater': 643, 'quiet': 644, 'stars': 645, 'beside': 646, 'warm': 647, 'sunday': 648, 'grey': 649, 'ocean': 650, 'sad': 651, 'spend': 652, 'kilkenny': 653, 'silver': 654, 'view': 655, 'west': 656, 'plain': 657, 'barrow': 658, 'broad': 659, 'narrow': 660, 'crying': 661, 'wonder': 662, 'save': 663, 'stop': 664, 'tender': 665, 'told': 666, 'lip': 667, 'dance': 668, 'foot': 669, 'kilrain': 670, 'saint': 671, 'visit': 672, 'mossy': 673, 'wexford': 674, 'irishmen': 675, 'shadow': 676, 'tho': 677, 'salley': 678, 'gardens': 679, 'foolish': 680, 'youth': 681, 'fade': 682, 'war': 683, 'believe': 684, 'which': 685, 'change': 686, 'entwine': 687, 'turns': 688, 'turned': 689, 'crown': 690, 'played': 691, 'captain': 692, 'blow': 693, 'children': 694, 'slainte': 695, 'gentle': 696, 'heavens': 697, 'bloom': 698, 'grand': 699, 'bush': 700, 'nest': 701, 'rich': 702, 'parting': 703, 'better': 704, 'window': 705, 'haste': 706, 'fresh': 707, 'stream': 708, 'rays': 709, 'ma': 710, 'ring': 711, 'lad': 712, 'athy': 713, 'drop': 714, 'hardly': 715, 'done': 716, 'arm': 717, 'leg': 718, 'beg': 719, 'drew': 720, 'bold': 721, 'drawn': 722, 'jail': 723, 'writin': 724, 'farewell': 725, 'tired': 726, 'lake': 727, 'want': 728, 'ringlets': 729, 'myself': 730, 'songs': 731, 'reel': 732, 'steps': 733, 'hearty': 734, 'fainted': 735, 'called': 736, 'under': 737, 'toe': 738, 'mairi': 739, 'fairest': 740, 'darlin': 741, 'bird': 742, 'memory': 743, 'lips': 744, 'sweetly': 745, 'morrow': 746, 'consent': 747, 'else': 748, 'sold': 749, 'stout': 750, 'pair': 751, 'drinking': 752, 'meself': 753, 'fray': 754, 'pike': 755, 'coat': 756, 'beneath': 757, 'rent': 758, 'part': 759, 'half': 760, 'head': 761, 'friend': 762, 'standing': 763, 'floor': 764, 'bare': 765, 'wed': 766, 'son': 767, 'pride': 768, 'vision': 769, 'sword': 770, 'after': 771, 'won': 772, 'farmers': 773, 'flower': 774, 'nut': 775, 'surely': 776, 'stood': 777, 'wandered': 778, 'athenry': 779, 'rising': 780, 'beating': 781, 'form': 782, 'dhu': 783, 'buy': 784, 'laughter': 785, 'wear': 786, 'raking': 787, 'rakes': 788, 'claret': 789, 'shure': 790, 'tralee': 791, 'slower': 792, 'lower': 793, 'deep': 794, 'wearin': 795, 'duram': 796, 'takes': 797, 'beware': 798, 'steal': 799, 'brings': 800, 'things': 801, 'joys': 802, 'bunch': 803, 'sailor': 804, 'chanced': 805, 'pass': 806, 'angels': 807, 'send': 808, 'drowsy': 809, 'keeping': 810, 'spirit': 811, 'stealing': 812, 'feeling': 813, 'roam': 814, 'presence': 815, 'heavenward': 816, 'dust': 817, 'dim': 818, 'journey': 819, 'waves': 820, 'frightened': 821, 'leaving': 822, 'struggle': 823, 'parents': 824, 'courage': 825, 'weeping': 826, 'pain': 827, 'mist': 828, 'felt': 829, 'roared': 830, 'making': 831, 'fever': 832, 'moment': 833, 'distance': 834, 'wailing': 835, 'oft': 836, 'held': 837, 'fast': 838, 'cabin': 839, 'honey': 840, 'diddle': 841, 'clearly': 842, 'open': 843, 'opened': 844, 'table': 845, 'wine': 846, 'lay': 847, 'shells': 848, 'sailed': 849, 'drown': 850, 'fetters': 851, 'chains': 852, 'wives': 853, 'sorrow': 854, 'thoughts': 855, 'cursed': 856, 'hell': 857, 'five': 858, 'buried': 859, 'lost': 860, 'endless': 861, 'slavery': 862, 'gun': 863, 'rain': 864, 'cares': 865, 'ghosts': 866, 'runaway': 867, 'twill': 868, 'month': 869, 'meadows': 870, 'prettiest': 871, 'winters': 872, 'satisfied': 873, 'few': 874, 'short': 875, 'lines': 876, 'shone': 877, 'shoulder': 878, 'belfast': 879, 'trade': 880, 'bad': 881, 'caused': 882, 'stray': 883, 'meaning': 884, 'damsel': 885, 'appear': 886, 'seven': 887, 'sentence': 888, 'jolly': 889, 'whenever': 890, 'wee': 891, 'wife': 892, 'lives': 893, 'martha': 894, 'courted': 895, 'bridgit': 896, 'omalley': 897, 'desolation': 898, 'thorn': 899, 'gaze': 900, 'stone': 901, 'approaching': 902, 'sets': 903, 'carrigfergus': 904, 'nights': 905, 'swim': 906, 'wings': 907, 'sober': 908, 'travel': 909, 'native': 910, 'places': 911, 'slopes': 912, 'hares': 913, 'lofty': 914, 'malone': 915, 'wheeled': 916, 'streets': 917, 'enough': 918, 'reilly': 919, 'tough': 920, 'whispers': 921, 'phil': 922, 'threw': 923, 'straight': 924, 'belles': 925, 'moor': 926, 'brand': 927, 'shapes': 928, 'work': 929, 'vow': 930, 'blarney': 931, 'paid': 932, 'bower': 933, 'remain': 934, 'charming': 935, 'storied': 936, 'chieftains': 937, 'slaughter': 938, 'bann': 939, 'boyne': 940, 'liffey': 941, 'gallant': 942, 'awake': 943, 'greet': 944, 'meadow': 945, 'sweeter': 946, 'dirty': 947, 'cats': 948, 'crossed': 949, 'field': 950, 'river': 951, 'full': 952, 'aroon': 953, 'sends': 954, 'woe': 955, 'chain': 956, 'main': 957, 'charms': 958, 'fondly': 959, 'fleet': 960, 'fairy': 961, 'thine': 962, 'known': 963, 'truly': 964, 'close': 965, 'story': 966, 'flag': 967, 'sweetest': 968, 'honor': 969, 'playing': 970, 'mauser': 971, 'music': 972, 'tom': 973, 'hurrah': 974, 'big': 975, 'lead': 976, 'south': 977, 'generation': 978, 'freedom': 979, 'agin': 980, 'creature': 981, 'dad': 982, 'venture': 983, 'word': 984, 'wonderful': 985, 'crazy': 986, 'lazy': 987, 'grave': 988, 'jest': 989, 'remark': 990, 'strangers': 991, 'strong': 992, 'shook': 993, 'walk': 994, 'north': 995, 'ours': 996, 'cease': 997, 'strife': 998, 'whats': 999, 'lilacs': 1000, 'prove': 1001, 'sweetheart': 1002, 'letters': 1003, 'sent': 1004, 'speak': 1005, 'brow': 1006, 'albert': 1007, 'mooney': 1008, 'fighting': 1009, 'fingers': 1010, 'toes': 1011, 'john': 1012, 'hurroo': 1013, 'drums': 1014, 'beguiled': 1015, 'carry': 1016, 'bone': 1017, 'havent': 1018, 'walkin': 1019, 'kilgary': 1020, 'pepper': 1021, 'countin': 1022, 'forth': 1023, 'deliver': 1024, 'daddy': 1025, 'em': 1026, 'deceive': 1027, 'between': 1028, 'even': 1029, 'prisoner': 1030, 'fists': 1031, 'knocked': 1032, 'carriages': 1033, 'rollin': 1034, 'juice': 1035, 'courtin': 1036, 'ponchartrain': 1037, 'does': 1038, 'stranger': 1039, 'marry': 1040, 'adieu': 1041, 'ask': 1042, 'tipped': 1043, 'arrived': 1044, 'ladies': 1045, 'potatoes': 1046, 'courting': 1047, 'miss': 1048, 'small': 1049, 'ned': 1050, 'ribbons': 1051, 'heel': 1052, 'bonny': 1053, 'pipe': 1054, 'thrush': 1055, 'sweethearts': 1056, 'unto': 1057, 'rise': 1058, 'softly': 1059, 'milking': 1060, 'rare': 1061, 'pity': 1062, 'treasure': 1063, 'noon': 1064, 'sailing': 1065, 'banish': 1066, 'riches': 1067, 'comfort': 1068, 'yonder': 1069, 'flows': 1070, 'fairer': 1071, 'lass': 1072, 'woods': 1073, 'strayed': 1074, 'locks': 1075, 'breaking': 1076, 'june': 1077, 'started': 1078, 'hearted': 1079, 'beer': 1080, 'daylight': 1081, 'among': 1082, 'bundle': 1083, 'connaught': 1084, 'quay': 1085, 'erins': 1086, 'galway': 1087, 'fearless': 1088, 'bravely': 1089, 'marches': 1090, 'fate': 1091, 'neck': 1092, 'trod': 1093, 'marched': 1094, 'antrim': 1095, 'sash': 1096, 'flashed': 1097, 'hath': 1098, 'foemans': 1099, 'fight': 1100, 'heavy': 1101, 'bore': 1102, 'mans': 1103, 'counter': 1104, 'dozen': 1105, 'gallon': 1106, 'bottles': 1107, 'diamond': 1108, 'resemble': 1109, 'tiny': 1110, 'friendly': 1111, 'weather': 1112, 'inside': 1113, 'remember': 1114, 'someone': 1115, 'hat': 1116, 'body': 1117, 'dancers': 1118, 'hanging': 1119, 'empty': 1120, 'shoes': 1121, 'broke': 1122, 'december': 1123, 'move': 1124, 'reason': 1125, 'roof': 1126, 'naught': 1127, 'tower': 1128, 'power': 1129, 'king': 1130, 'dreaming': 1131, 'crew': 1132, 'whos': 1133, 'mccann': 1134, 'smoke': 1135, 'notes': 1136, 'yeoman': 1137, 'cavalry': 1138, 'guard': 1139, 'forced': 1140, 'brother': 1141, 'cousin': 1142, 'blame': 1143, 'croppy': 1144, 'dressed': 1145, 'trees': 1146, 'wore': 1147, 'words': 1148, 'swiftly': 1149, 'dawn': 1150, 'lovd': 1151, 'voices': 1152, 'moaning': 1153, 'dark': 1154, 'gather': 1155, 'tay': 1156, 'swinging': 1157, 'drinkin': 1158, 'sitting': 1159, 'stile': 1160, 'springing': 1161, 'yours': 1162, 'kept': 1163, 'aisey': 1164, 'rub': 1165, 'dub': 1166, 'dow': 1167, 'shelah': 1168, 'fairly': 1169, 'beggarman': 1170, 'begging': 1171, 'slept': 1172, 'holes': 1173, 'coming': 1174, 'thru': 1175, 'boo': 1176, 'lady': 1177, 'kerry': 1178, 'pipers': 1179, 'laugh': 1180, 'beaming': 1181, 'guineas': 1182, 'least': 1183, 'diggin': 1184, 'mourne': 1185, 'spending': 1186, 'mellow': 1187, 'plying': 1188, 'slowly': 1189, 'mooncoin': 1190, 'flow': 1191, 'sounds': 1192, 'shine': 1193, 'cool': 1194, 'crystal': 1195, 'fountain': 1196, 'moonlight': 1197, 'grandmother': 1198, 'crooning': 1199, 'merrily': 1200, 'spins': 1201, 'lightly': 1202, 'moving': 1203, 'lattice': 1204, 'grove': 1205, 'swings': 1206, 'finger': 1207, 'shamrock': 1208, 'pocket': 1209, 'springtime': 1210, 'gilgarra': 1211, 'rapier': 1212, 'ringum': 1213, 'mornin': 1214, 'heather': 1215, 'build': 1216, 'maidens': 1217, 'prime': 1218, 'nlyme': 1219, 'flavours': 1220, 'lusty': 1221, 'reminded': 1222, 'attend': 1223, 'guardian': 1224, 'creeping': 1225, 'dale': 1226, 'vigil': 1227, 'visions': 1228, 'revealing': 1229, 'breathes': 1230, 'holy': 1231, 'strains': 1232, 'hover': 1233, 'hark': 1234, 'solemn': 1235, 'winging': 1236, 'earthly': 1237, 'shalt': 1238, 'awaken': 1239, 'destiny': 1240, 'emigrants': 1241, 'amid': 1242, 'longing': 1243, 'parted': 1244, 'townland': 1245, 'vessel': 1246, 'crowded': 1247, 'disquieted': 1248, 'folk': 1249, 'escape': 1250, 'hardship': 1251, 'sustaining': 1252, 'glimpse': 1253, 'faded': 1254, 'strangely': 1255, 'seas': 1256, 'anger': 1257, 'desperate': 1258, 'plight': 1259, 'worsened': 1260, 'delirium': 1261, 'possessed': 1262, 'clouded': 1263, 'prayers': 1264, 'begged': 1265, 'forgiveness': 1266, 'seeking': 1267, 'distant': 1268, 'mither': 1269, 'simple': 1270, 'ditty': 1271, 'ld': 1272, 'li': 1273, 'hush': 1274, 'lullaby': 1275, 'huggin': 1276, 'hummin': 1277, 'rock': 1278, 'asleep': 1279, 'outside': 1280, 'modestly': 1281, 'ry': 1282, 'ay': 1283, 'di': 1284, 're': 1285, 'dai': 1286, 'rie': 1287, 'shc': 1288, 'bridle': 1289, 'stable': 1290, 'oats': 1291, 'eat': 1292, 'soldier': 1293, 'aisy': 1294, 'arose': 1295, 'christmas': 1296, '1803': 1297, 'australia': 1298, 'marks': 1299, 'carried': 1300, 'rusty': 1301, 'iron': 1302, 'wains': 1303, 'mainsails': 1304, 'unfurled': 1305, 'curses': 1306, 'hurled': 1307, 'swell': 1308, 'moth': 1309, 'firelights': 1310, 'horses': 1311, 'rode': 1312, 'taking': 1313, 'hades': 1314, 'twilight': 1315, 'forty': 1316, 'slime': 1317, 'climate': 1318, 'bravery': 1319, 'ended': 1320, 'bond': 1321, 'rebel': 1322, 'iii': 1323, 'violin': 1324, 'clay': 1325, 'sooner': 1326, 'sport': 1327, 'colour': 1328, 'knows': 1329, 'earth': 1330, 'serve': 1331, 'clyde': 1332, 'mourn': 1333, 'weep': 1334, 'suffer': 1335, 'diamonds': 1336, 'queen': 1337, 'hung': 1338, 'tied': 1339, 'apprenticed': 1340, 'happiness': 1341, 'misfortune': 1342, 'follow': 1343, 'strolling': 1344, 'selling': 1345, 'bar': 1346, 'customer': 1347, 'slipped': 1348, 'luck': 1349, 'jury': 1350, 'trial': 1351, 'case': 1352, 'warning': 1353, 'liquor': 1354, 'porter': 1355, 'pleasures': 1356, 'fishing': 1357, 'farming': 1358, 'glens': 1359, 'softest': 1360, 'dripping': 1361, 'snare': 1362, 'lose': 1363, 'court': 1364, 'primrose': 1365, 'bee': 1366, 'hopeless': 1367, 'wonders': 1368, 'admiration': 1369, 'haunt': 1370, 'wherever': 1371, 'sands': 1372, 'purer': 1373, 'within': 1374, 'grieve': 1375, 'drumslieve': 1376, 'ballygrant': 1377, 'deepest': 1378, 'boatsman': 1379, 'ferry': 1380, 'childhood': 1381, 'reflections': 1382, 'boyhood': 1383, 'melting': 1384, 'roaming': 1385, 'reported': 1386, 'marble': 1387, 'stones': 1388, 'ink': 1389, 'support': 1390, 'drunk': 1391, 'seldom': 1392, 'sick': 1393, 'numbered': 1394, 'foam': 1395, 'compare': 1396, 'sights': 1397, 'coast': 1398, 'clare': 1399, 'kilkee': 1400, 'kilrush': 1401, 'watching': 1402, 'pheasants': 1403, 'homes': 1404, 'streams': 1405, 'dublins': 1406, 'cockles': 1407, 'mussels': 1408, 'fish': 1409, 'monger': 1410, 'ghost': 1411, 'wheels': 1412, 'eden': 1413, 'vanished': 1414, 'finea': 1415, 'halfway': 1416, 'cootehill': 1417, 'gruff': 1418, 'whispering': 1419, 'crow': 1420, 'newborn': 1421, 'babies': 1422, 'huff': 1423, 'start': 1424, 'sorrowful': 1425, 'squall': 1426, 'babys': 1427, 'toil': 1428, 'worn': 1429, 'fore': 1430, 'flute': 1431, 'yer': 1432, 'boot': 1433, 'magee': 1434, 'scruff': 1435, 'slanderin': 1436, 'marchin': 1437, 'assisted': 1438, 'drain': 1439, 'dudeen': 1440, 'puff': 1441, 'whisperings': 1442, 'barrin': 1443, 'chocolate': 1444, 'feegee': 1445, 'sort': 1446, 'moonshiny': 1447, 'stuff': 1448, 'addle': 1449, 'brain': 1450, 'ringin': 1451, 'glamour': 1452, 'gas': 1453, 'guff': 1454, 'whisper': 1455, 'oil': 1456, 'remarkable': 1457, 'policeman': 1458, 'bluff': 1459, 'maintain': 1460, 'guril': 1461, 'sic': 1462, 'passage': 1463, 'rough': 1464, 'borne': 1465, 'breeze': 1466, 'boundless': 1467, 'stupendous': 1468, 'roll': 1469, 'thundering': 1470, 'motion': 1471, 'mermaids': 1472, 'fierce': 1473, 'tempest': 1474, 'gathers': 1475, 'oneill': 1476, 'odonnell': 1477, 'lucan': 1478, 'oconnell': 1479, 'brian': 1480, 'drove': 1481, 'danes': 1482, 'patrick': 1483, 'vermin': 1484, 'whose': 1485, 'benburb': 1486, 'blackwater': 1487, 'owen': 1488, 'roe': 1489, 'munroe': 1490, 'lambs': 1491, 'skip': 1492, 'views': 1493, 'enchanting': 1494, 'rostrevor': 1495, 'groves': 1496, 'lakes': 1497, 'ride': 1498, 'tide': 1499, 'majestic': 1500, 'shannon': 1501, 'sail': 1502, 'loch': 1503, 'neagh': 1504, 'ross': 1505, 'gorey': 1506, 'saxon': 1507, 'tory': 1508, 'soil': 1509, 'sanctified': 1510, 'enemies': 1511, 'links': 1512, 'encumbered': 1513, 'resound': 1514, 'hosannahs': 1515, 'bide': 1516, 'hushed': 1517, 'lying': 1518, 'kneel': 1519, 'ave': 1520, 'tread': 1521, 'fail': 1522, 'simply': 1523, 'gasworks': 1524, 'croft': 1525, 'dreamed': 1526, 'canal': 1527, 'factory': 1528, 'clouds': 1529, 'drifting': 1530, 'prowling': 1531, 'beat': 1532, 'springs': 1533, 'siren': 1534, 'docks': 1535, 'train': 1536, 'smelled': 1537, 'smokey': 1538, 'sharp': 1539, 'axe': 1540, 'steel': 1541, 'tempered': 1542, 'chop': 1543, 't': 1544, 'agree': 1545, 'leaning': 1546, 'weirs': 1547, 'ray': 1548, 'glow': 1549, 'changeless': 1550, 'constant': 1551, 'bounding': 1552, 'castles': 1553, 'sacked': 1554, 'scattered': 1555, 'fixed': 1556, 'endearing': 1557, 'gifts': 1558, 'fading': 1559, 'wouldst': 1560, 'adored': 1561, 'loveliness': 1562, 'ruin': 1563, 'itself': 1564, 'verdantly': 1565, 'unprofaned': 1566, 'fervor': 1567, 'faith': 1568, 'forgets': 1569, 'sunflower': 1570, 'rag': 1571, 'games': 1572, 'hold': 1573, 'defend': 1574, 'veteran': 1575, 'volunteers': 1576, 'pat': 1577, 'pearse': 1578, 'clark': 1579, 'macdonagh': 1580, 'macdiarmada': 1581, 'mcbryde': 1582, 'james': 1583, 'connolly': 1584, 'placed': 1585, 'machine': 1586, 'ranting': 1587, 'hour': 1588, 'bullet': 1589, 'stuck': 1590, 'craw': 1591, 'poisoning': 1592, 'ceannt': 1593, 'lions': 1594, 'union': 1595, 'poured': 1596, 'dismay': 1597, 'horror': 1598, 'englishmen': 1599, 'khaki': 1600, 'renown': 1601, 'fame': 1602, 'forefathers': 1603, 'blaze': 1604, 'priests': 1605, 'offer': 1606, 'charmin': 1607, 'variety': 1608, 'renownd': 1609, 'learnin': 1610, 'piety': 1611, 'advance': 1612, 'widout': 1613, 'impropriety': 1614, 'flowr': 1615, 'cho': 1616, 'powrfulest': 1617, 'preacher': 1618, 'tenderest': 1619, 'teacher': 1620, 'kindliest': 1621, 'donegal': 1622, 'talk': 1623, 'provost': 1624, 'trinity': 1625, 'famous': 1626, 'greek': 1627, 'latinity': 1628, 'divils': 1629, 'divinity': 1630, 'd': 1631, 'likes': 1632, 'logic': 1633, 'mythology': 1634, 'thayology': 1635, 'conchology': 1636, 'sinners': 1637, 'wishful': 1638, 'childer': 1639, 'avick': 1640, 'gad': 1641, 'flock': 1642, 'grandest': 1643, 'control': 1644, 'checking': 1645, 'coaxin': 1646, 'onaisy': 1647, 'lifting': 1648, 'avoidin': 1649, 'frivolity': 1650, 'seasons': 1651, 'innocent': 1652, 'jollity': 1653, 'playboy': 1654, 'claim': 1655, 'equality': 1656, 'comicality': 1657, 'bishop': 1658, 'lave': 1659, 'gaiety': 1660, 'laity': 1661, 'clergy': 1662, 'jewels': 1663, 'plundering': 1664, 'pillage': 1665, 'starved': 1666, 'cries': 1667, 'thems': 1668, 'bondage': 1669, 'fourth': 1670, 'tabhair': 1671, 'dom': 1672, 'lmh': 1673, 'harmony': 1674, 'east': 1675, 'destroy': 1676, 'command': 1677, 'gesture': 1678, 'troubles': 1679, 'weak': 1680, 'peoples': 1681, 'creeds': 1682, 'lets': 1683, 'needs': 1684, 'passion': 1685, 'fashion': 1686, 'guide': 1687, 'share': 1688, 'sparkling': 1689, 'meeting': 1690, 'iull': 1691, 'contented': 1692, 'ache': 1693, 'painful': 1694, 'wrote': 1695, 'twisted': 1696, 'twined': 1697, 'cheek': 1698, 'bedim': 1699, 'holds': 1700, 'smiles': 1701, 'scarcely': 1702, 'darkning': 1703, 'beyond': 1704, 'yearn': 1705, 'laughs': 1706, 'humble': 1707, 'brightest': 1708, 'gleam': 1709, 'forgot': 1710, 'pulled': 1711, 'comb': 1712, 'counting': 1713, 'knock': 1714, 'murray': 1715, 'fellow': 1716, 'hail': 1717, 'tumblin': 1718, 'apple': 1719, 'pie': 1720, 'gets': 1721, 'doleful': 1722, 'enemy': 1723, 'nearly': 1724, 'slew': 1725, 'queer': 1726, 'mild': 1727, 'legs': 1728, 'indeed': 1729, 'island': 1730, 'sulloon': 1731, 'flesh': 1732, 'yere': 1733, 'armless': 1734, 'boneless': 1735, 'chickenless': 1736, 'egg': 1737, 'yell': 1738, 'bowl': 1739, 'rolling': 1740, 'swearing': 1741, 'rattled': 1742, 'saber': 1743, 'deceiver': 1744, 'rig': 1745, 'um': 1746, 'du': 1747, 'rum': 1748, 'jar': 1749, 'shinin': 1750, 'coins': 1751, 'promised': 1752, 'vowed': 1753, 'devils': 1754, 'awakened': 1755, 'six': 1756, 'guards': 1757, 'numbers': 1758, 'odd': 1759, 'flew': 1760, 'mistaken': 1761, 'mollys': 1762, 'robbing': 1763, 'sentry': 1764, 'sligo': 1765, 'fishin': 1766, 'bowlin': 1767, 'others': 1768, 'railroad': 1769, 'ties': 1770, 'crossings': 1771, 'swamps': 1772, 'elevations': 1773, 'resolved': 1774, 'sunset': 1775, 'higher': 1776, 'win': 1777, 'allegators': 1778, 'wood': 1779, 'treated': 1780, 'shoulders': 1781, 'paint': 1782, 'picture': 1783, 'vain': 1784, 'returned': 1785, 'cottage': 1786, 'sociable': 1787, 'foaming': 1788, 'n': 1789, 'jeremy': 1790, 'lanigan': 1791, 'battered': 1792, 'hadnt': 1793, 'pound': 1794, 'farm': 1795, 'acres': 1796, 'party': 1797, 'listen': 1798, 'glisten': 1799, 'rows': 1800, 'ructions': 1801, 'invitation': 1802, 'minute': 1803, 'bees': 1804, 'cask': 1805, 'judy': 1806, 'odaly': 1807, 'milliner': 1808, 'wink': 1809, 'peggy': 1810, 'mcgilligan': 1811, 'lashings': 1812, 'punch': 1813, 'cakes': 1814, 'bacon': 1815, 'tea': 1816, 'nolans': 1817, 'dolans': 1818, 'ogradys': 1819, 'sounded': 1820, 'taras': 1821, 'hall': 1822, 'nelly': 1823, 'gray': 1824, 'rat': 1825, 'catchers': 1826, 'doing': 1827, 'kinds': 1828, 'nonsensical': 1829, 'polkas': 1830, 'whirligig': 1831, 'julia': 1832, 'banished': 1833, 'nonsense': 1834, 'twist': 1835, 'jig': 1836, 'mavrone': 1837, 'mad': 1838, 'ceiling': 1839, 'brooks': 1840, 'academy': 1841, 'learning': 1842, 'learn': 1843, 'couples': 1844, 'groups': 1845, 'accident': 1846, 'happened': 1847, 'terrance': 1848, 'mccarthy': 1849, 'finnertys': 1850, 'hoops': 1851, 'cried': 1852, 'meelia': 1853, 'murther': 1854, 'gathered': 1855, 'carmody': 1856, 'further': 1857, 'satisfaction': 1858, 'midst': 1859, 'kerrigan': 1860, 'declared': 1861, 'painted': 1862, 'suppose': 1863, 'morgan': 1864, 'powerful': 1865, 'stretched': 1866, 'smashed': 1867, 'chaneys': 1868, 'runctions': 1869, 'lick': 1870, 'phelim': 1871, 'mchugh': 1872, 'replied': 1873, 'introduction': 1874, 'kicked': 1875, 'terrible': 1876, 'hullabaloo': 1877, 'piper': 1878, 'strangled': 1879, 'squeezed': 1880, 'bellows': 1881, 'chanters': 1882, 'entangled': 1883, 'gaily': 1884, 'mairis': 1885, 'hillways': 1886, 'myrtle': 1887, 'bracken': 1888, 'sheilings': 1889, 'sake': 1890, 'rowans': 1891, 'herring': 1892, 'meal': 1893, 'peat': 1894, 'creel': 1895, 'bairns': 1896, 'weel': 1897, 'toast': 1898, 'soar': 1899, 'blackbird': 1900, 'note': 1901, 'linnet': 1902, 'lure': 1903, 'cozy': 1904, 'catch': 1905, 'company': 1906, 'harm': 1907, 'wit': 1908, 'recall': 1909, 'leisure': 1910, 'awhile': 1911, 'sorely': 1912, 'ruby': 1913, 'enthralled': 1914, 'sorry': 1915, 'theyd': 1916, 'falls': 1917, 'lot': 1918, 'tuned': 1919, 'bough': 1920, 'cow': 1921, 'chanting': 1922, 'melodious': 1923, 'scarce': 1924, 'soothed': 1925, 'solace': 1926, 'courtesy': 1927, 'salute': 1928, 'amiable': 1929, 'captive': 1930, 'slave': 1931, 'future': 1932, 'banter': 1933, 'enamour': 1934, 'indies': 1935, 'afford': 1936, 'transparently': 1937, 'flame': 1938, 'add': 1939, 'fuel': 1940, 'grant': 1941, 'desire': 1942, 'expire': 1943, 'wealth': 1944, 'damer': 1945, 'african': 1946, 'devonshire': 1947, 'lamp': 1948, 'alladin': 1949, 'genie': 1950, 'also': 1951, 'withdraw': 1952, 'tease': 1953, 'single': 1954, 'airy': 1955, 'embarrass': 1956, 'besides': 1957, 'almanack': 1958, 'useless': 1959, 'date': 1960, 'ware': 1961, 'rate': 1962, 'fragrance': 1963, 'loses': 1964, 'consumed': 1965, 'october': 1966, 'knowing': 1967, 'steer': 1968, 'blast': 1969, 'danger': 1970, 'farthing': 1971, 'affection': 1972, 'enjoy': 1973, 'choose': 1974, 'killarneys': 1975, 'sister': 1976, 'pains': 1977, 'loss': 1978, 'tuam': 1979, 'saluted': 1980, 'drank': 1981, 'pint': 1982, 'smother': 1983, 'reap': 1984, 'cut': 1985, 'goblins': 1986, 'bought': 1987, 'brogues': 1988, 'rattling': 1989, 'bogs': 1990, 'frightning': 1991, 'dogs': 1992, 'hunt': 1993, 'hare': 1994, 'follol': 1995, 'rah': 1996, 'mullingar': 1997, 'rested': 1998, 'limbs': 1999, 'blithe': 2000, 'heartfrom': 2001, 'paddys': 2002, 'cure': 2003, 'lassies': 2004, 'laughing': 2005, 'curious': 2006, 'style': 2007, 'twould': 2008, 'bubblin': 2009, 'hired': 2010, 'wages': 2011, 'required': 2012, 'almost': 2013, 'deprived': 2014, 'stroll': 2015, 'quality': 2016, 'locality': 2017, 'something': 2018, 'wobblin': 2019, 'enquiring': 2020, 'rogue': 2021, 'brogue': 2022, 'wasnt': 2023, 'vogue': 2024, 'spirits': 2025, 'falling': 2026, 'jumped': 2027, 'aboard': 2028, 'pigs': 2029, 'rigs': 2030, 'jigs': 2031, 'bubbling': 2032, 'holyhead': 2033, 'wished': 2034, 'instead': 2035, 'bouys': 2036, 'liverpool': 2037, 'safely': 2038, 'fool': 2039, 'boil': 2040, 'temper': 2041, 'losing': 2042, 'abusing': 2043, 'shillelagh': 2044, 'nigh': 2045, 'hobble': 2046, 'load': 2047, 'hurray': 2048, 'joined': 2049, 'affray': 2050, 'quitely': 2051, 'cleared': 2052, 'host': 2053, 'march': 2054, 'faces': 2055, 'farmstead': 2056, 'fishers': 2057, 'ban': 2058, 'vengeance': 2059, 'hapless': 2060, 'about': 2061, 'hemp': 2062, 'rope': 2063, 'clung': 2064, 'grim': 2065, 'array': 2066, 'earnest': 2067, 'stalwart': 2068, 'stainless': 2069, 'banner': 2070, 'marching': 2071, 'torn': 2072, 'furious': 2073, 'odds': 2074, 'keen': 2075, 'toomebridge': 2076, 'treads': 2077, 'upwards': 2078, 'traveled': 2079, 'quarters': 2080, 'below': 2081, 'hogshead': 2082, 'stack': 2083, 'stagger': 2084, 'dig': 2085, 'hole': 2086, 'couple': 2087, 'scratch': 2088, 'consolation': 2089, 'tyrant': 2090, 'remorseless': 2091, 'foe': 2092, 'lift': 2093, 'stranded': 2094, 'prince': 2095, 'edward': 2096, 'coffee': 2097, 'trace': 2098, 'fiddlin': 2099, 'dime': 2100, 'shy': 2101, 'hello': 2102, 'wintry': 2103, 'yellow': 2104, 'somewhere': 2105, 'written': 2106, 'begin': 2107, 'tap': 2108, 'caught': 2109, 'leap': 2110, 'clumsy': 2111, 'graceful': 2112, 'fiddlers': 2113, 'everywhere': 2114, 'boots': 2115, 'laughtcr': 2116, 'suits': 2117, 'easter': 2118, 'gowns': 2119, 'sailors': 2120, 'pianos': 2121, 'setting': 2122, 'someones': 2123, 'hats': 2124, 'rack': 2125, 'chair': 2126, 'wooden': 2127, 'feels': 2128, 'touch': 2129, 'awaitin': 2130, 'thc': 2131, 'fiddles': 2132, 'closet': 2133, 'strings': 2134, 'tbe': 2135, 'covers': 2136, 'buttoned': 2137, 'sometimes': 2138, 'melody': 2139, 'passes': 2140, 'slight': 2141, 'lack': 2142, 'moved': 2143, 'homeward': 2144, 'swan': 2145, 'moves': 2146, 'goods': 2147, 'gear': 2148, 'din': 2149, 'rude': 2150, 'wherein': 2151, 'dwell': 2152, 'abandon': 2153, 'energy': 2154, 'blight': 2155, 'praties': 2156, 'sheep': 2157, 'cattle': 2158, 'taxes': 2159, 'unpaid': 2160, 'redeem': 2161, 'bleak': 2162, 'landlord': 2163, 'sheriff': 2164, 'spleen': 2165, 'heaved': 2166, 'sigh': 2167, 'bade': 2168, 'goodbye': 2169, 'stony': 2170, 'anguish': 2171, 'seeing': 2172, 'feeble': 2173, 'frame': 2174, 'wrapped': 2175, 'cta': 2176, 'mr': 2177, 'unseen': 2178, 'stern': 2179, 'rally': 2180, 'cheer': 2181, 'revenge': 2182, 'waking': 2183, 'wisdom': 2184, 'dwelling': 2185, 'battleshield': 2186, 'dignity': 2187, 'shelter': 2188, 'heed': 2189, 'inheritance': 2190, 'heavem': 2191, 'heaven': 2192, 'victory': 2193, 'reach': 2194, 'whatever': 2195, 'befall': 2196, 'ruler': 2197, 'pleasant': 2198, 'rambling': 2199, 'board': 2200, 'followed': 2201, 'shortly': 2202, 'anchor': 2203, '23rd': 2204, 'lrelands': 2205, 'daughters': 2206, 'crowds': 2207, 'assembled': 2208, 'fulfill': 2209, 'jovial': 2210, 'conversations': 2211, 'neighbors': 2212, 'turning': 2213, 'tailor': 2214, 'quigley': 2215, 'bould': 2216, 'britches': 2217, 'lived': 2218, 'flying': 2219, 'dove': 2220, 'hiii': 2221, 'dreamt': 2222, 'joking': 2223, 'manys': 2224, 'cock': 2225, 'shrill': 2226, 'awoke': 2227, 'california': 2228, 'miles': 2229, 'banbridge': 2230, 'july': 2231, 'boreen': 2232, 'sheen': 2233, 'coaxing': 2234, 'elf': 2235, 'shake': 2236, 'bantry': 2237, 'onward': 2238, 'sped': 2239, 'gazed': 2240, 'passerby': 2241, 'gem': 2242, 'irelands': 2243, 'travelled': 2244, 'hit': 2245, 'career': 2246, 'square': 2247, 'surrendered': 2248, 'tenant': 2249, 'shawl': 2250, 'gown': 2251, 'crossroads': 2252, 'dress': 2253, 'try': 2254, 'sheeps': 2255, 'deludhering': 2256, 'yoke': 2257, 'rust': 2258, 'plow': 2259, 'fireside': 2260, 'sits': 2261, 'whistle': 2262, 'changing': 2263, 'fright': 2264, 'downfall': 2265, 'cornwall': 2266, 'parlour': 2267, 'passing': 2268, 'william': 2269, 'betray': 2270, 'guinea': 2271, 'walking': 2272, 'mounted': 2273, 'platform': 2274, 'deny': 2275, 'walked': 2276, 'margin': 2277, 'lough': 2278, 'leane': 2279, 'bloomed': 2280, 'whom': 2281, 'cap': 2282, 'cloak': 2283, 'glossy': 2284, 'pail': 2285, 'palm': 2286, 'venus': 2287, 'bank': 2288, 'travelians': 2289, 'babes': 2290, 'freebirds': 2291, 'grew': 2292, 'matters': 2293, 'famine': 2294, 'rebelled': 2295, 'windswept': 2296, 'harbour': 2297, 'botany': 2298, 'whilst': 2299, 'wan': 2300, 'cloud': 2301, 'shannons': 2302, 'returnd': 2303, 'doubts': 2304, 'fears': 2305, 'aching': 2306, 'seemd': 2307, 'mingling': 2308, 'flood': 2309, 'path': 2310, 'wrath': 2311, 'lamenting': 2312, 'sudden': 2313, 'kissd': 2314, 'showrs': 2315, 'flowing': 2316, 'laughd': 2317, 'beam': 2318, 'soared': 2319, 'aloft': 2320, 'phantom': 2321, 'outspread': 2322, 'throbbing': 2323, 'hid': 2324, 'treasures': 2325, 'pots': 2326, 'tin': 2327, 'cans': 2328, 'mash': 2329, 'bran': 2330, 'barney': 2331, 'peeled': 2332, 'searching': 2333, 'connemara': 2334, 'butcher': 2335, 'quart': 2336, 'bottle': 2337, 'help': 2338, 'gate': 2339, 'glory': 2340, 'lane': 2341, 'village': 2342, 'church': 2343, 'spire': 2344, 'graveyard': 2345, 'baby': 2346, 'blessing': 2347, 'hoping': 2348, 'trust': 2349, 'strength': 2350, 'thank': 2351, 'bidding': 2352, 'bread': 2353, 'shines': 2354, 'fifty': 2355, 'often': 2356, 'shut': 2357, 'frisky': 2358, 'pig': 2359, 'whisky': 2360, 'uncle': 2361, 'enlisted': 2362, 'trudged': 2363, 'bosom': 2364, 'daisy': 2365, 'drubbing': 2366, 'shirts': 2367, 'battle': 2368, 'blows': 2369, 'pate': 2370, 'bothered': 2371, 'rarely': 2372, 'dropped': 2373, 'honest': 2374, 'thinks': 2375, 'eight': 2376, 'score': 2377, 'basin': 2378, 'zoo': 2379, 'everybody': 2380, 'calls': 2381, 'trades': 2382, 'dinner': 2383, 'slip': 2384, 'corner': 2385, 'barn': 2386, 'currabawn': 2387, 'shocking': 2388, 'wet': 2389, 'raindrops': 2390, 'rats': 2391, 'peek': 2392, 'waken': 2393, 'spotted': 2394, 'apron': 2395, 'calico': 2396, 'blouse': 2397, 'frighten': 2398, 'afraid': 2399, 'flaxen': 2400, 'haired': 2401, 'rags': 2402, 'tags': 2403, 'leggins': 2404, 'collar': 2405, 'tie': 2406, 'goggles': 2407, 'fashioned': 2408, 'bag': 2409, 'bulging': 2410, 'sack': 2411, 'peeping': 2412, 'skin': 2413, 'rink': 2414, 'doodle': 2415, 'getting': 2416, 'raked': 2417, 'gladness': 2418, 'tuning': 2419, 'fills': 2420, 'eily': 2421, 'prouder': 2422, 'thady': 2423, 'boldly': 2424, 'lasses': 2425, 'fled': 2426, 'silent': 2427, 'glad': 2428, 'echo': 2429, 'companions': 2430, 'soars': 2431, 'enchanted': 2432, 'granted': 2433, 'adoration': 2434, 'gives': 2435, 'joyous': 2436, 'elation': 2437, 'covered': 2438, 'winter': 2439, 'riding': 2440, 'cherry': 2441, 'coal': 2442, 'falter': 2443, 'bowed': 2444, 'bonnet': 2445, 'courteous': 2446, 'looks': 2447, 'engaging': 2448, 'sell': 2449, 'purse': 2450, 'yearly': 2451, 'need': 2452, 'market': 2453, 'gain': 2454, 'dearly': 2455, 'tarry': 2456, 'although': 2457, 'parlay': 2458, 'ranks': 2459, 'girded': 2460, 'slung': 2461, 'warrior': 2462, 'bard': 2463, 'betrays': 2464, 'rights': 2465, 'faithful': 2466, 'chords': 2467, 'asunder': 2468, 'sully': 2469, 'bravry': 2470, 'londons': 2471, 'sight': 2472, 'workin': 2473, 'sow': 2474, 'wheat': 2475, 'gangs': 2476, 'sweep': 2477, 'expressed': 2478, 'london': 2479, 'top': 2480, 'dresses': 2481, 'bath': 2482, 'startin': 2483, 'fashions': 2484, 'mccree': 2485, 'nature': 2486, 'designed': 2487, 'complexions': 2488, 'cream': 2489, 'regard': 2490, 'sip': 2491, 'colors': 2492, 'wait': 2493, 'waitin': 2494, 'sweeps': 2495, 'beauing': 2496, 'belling': 2497, 'windows': 2498, 'cursing': 2499, 'faster': 2500, 'waiters': 2501, 'bailiffs': 2502, 'duns': 2503, 'bacchus': 2504, 'begotten': 2505, 'politicians': 2506, 'funds': 2507, 'dadda': 2508, 'living': 2509, 'drives': 2510, 'having': 2511, 'racking': 2512, 'tenants': 2513, 'stewards': 2514, 'teasing': 2515, 'raising': 2516, 'wishing': 2517, 'sunny': 2518, 'doves': 2519, 'coo': 2520, 'neath': 2521, 'sunbeam': 2522, 'robin': 2523, 'waters': 2524, 'larks': 2525, 'join': 2526, 'breaks': 2527, 'oftimes': 2528, 'lilies': 2529, 'declining': 2530, 'vale': 2531, 'shades': 2532, 'mantle': 2533, 'spreading': 2534, 'listening': 2535, 'shedding': 2536, 'beginning': 2537, 'spinning': 2538, 'blind': 2539, 'drowsily': 2540, 'knitting': 2541, 'cheerily': 2542, 'noiselessly': 2543, 'whirring': 2544, 'foots': 2545, 'stirring': 2546, 'sprightly': 2547, 'chara': 2548, 'tapping': 2549, 'ivy': 2550, 'flapping': 2551, 'somebody': 2552, 'sighing': 2553, 'autumn': 2554, 'noise': 2555, 'chirping': 2556, 'holly': 2557, 'shoving': 2558, 'wrong': 2559, 'coolin': 2560, 'casement': 2561, 'rove': 2562, 'moons': 2563, 'brightly': 2564, 'shakes': 2565, 'lays': 2566, 'longs': 2567, 'lingers': 2568, 'glance': 2569, 'puts': 2570, 'lazily': 2571, 'easily': 2572, 'lowly': 2573, 'reels': 2574, 'noiseless': 2575, 'leaps': 2576, 'ere': 2577, 'lovers': 2578, 'roved': 2579, 'verdant': 2580, 'braes': 2581, 'skreen': 2582, 'countrie': 2583, 'foreign': 2584, 'strand': 2585, 'dewy': 2586, 'climb': 2587, 'rob': 2588, 'boat': 2589, 'sails': 2590, 'loaded': 2591, 'sink': 2592, 'leaned': 2593, 'oak': 2594, 'trusty': 2595, 'false': 2596, 'reached': 2597, 'pricked': 2598, 'waxes': 2599, 'fades': 2600, 'wholl': 2601, 'cockle': 2602, 'gloom': 2603, 'news': 2604, 'forbid': 2605, 'patricks': 2606, 'napper': 2607, 'tandy': 2608, 'hows': 2609, 'distressful': 2610, 'englands': 2611, 'remind': 2612, 'pull': 2613, 'throw': 2614, 'sod': 2615, 'root': 2616, 'underfoot': 2617, 'laws': 2618, 'blades': 2619, 'growin': 2620, 'dare': 2621, 'show': 2622, 'caubeen': 2623, 'year': 2624, 'returning': 2625, 'store': 2626, 'ale': 2627, 'frequent': 2628, 'landlady': 2629, 'credit': 2630, 'custom': 2631, 'sovereigns': 2632, 'landladys': 2633, 'wines': 2634, 'confess': 2635, 'pardon': 2636, 'prodigal': 2637, 'caress': 2638, 'forgive': 2639, 'ofttimes': 2640, 'wondering': 2641, 'powr': 2642, 'beguile': 2643, 'teardrop': 2644, 'lilting': 2645, 'laughters': 2646, 'twinkle': 2647, 'lilt': 2648, 'seems': 2649, 'linnets': 2650, 'real': 2651, 'regret': 2652, 'throughout': 2653, 'youths': 2654, 'chance': 2655, 'spied': 2656, 'receiver': 2657, 'counted': 2658, 'penny': 2659, 'bu': 2660, 'rungum': 2661, 'chamber': 2662, 'course': 2663, 'charges': 2664, 'filled': 2665, 'ready': 2666, 'footmen': 2667, 'likewise': 2668, 'draw': 2669, 'pistol': 2670, 'couldnt': 2671, 'shoot': 2672, 'robbin': 2673, 'jailer': 2674, 'tight': 2675, 'fisted': 2676, 'army': 2677, 'stationed': 2678, 'cork': 2679, 'roamin': 2680, 'swear': 2681, 'treat': 2682, 'sportin': 2683, 'hurley': 2684, 'bollin': 2685, 'maids': 2686, 'summertime': 2687, 'pluck': 2688, 'yon': 2689}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eQoxG9DTj59",
        "outputId": "b8333e78-dd82-4137-b779-26368c051c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
        "#print model.summary()\n",
        "print(model)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "377/377 [==============================] - 4s 11ms/step - loss: 6.6490 - accuracy: 0.0757\n",
            "Epoch 2/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 5.7650 - accuracy: 0.1156\n",
            "Epoch 3/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 4.9026 - accuracy: 0.1614\n",
            "Epoch 4/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 4.0266 - accuracy: 0.2330\n",
            "Epoch 5/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 3.2539 - accuracy: 0.3209\n",
            "Epoch 6/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 2.5538 - accuracy: 0.4305\n",
            "Epoch 7/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 2.0730 - accuracy: 0.5194\n",
            "Epoch 8/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.7037 - accuracy: 0.5954\n",
            "Epoch 9/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 1.4551 - accuracy: 0.6459\n",
            "Epoch 10/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.3258 - accuracy: 0.6796\n",
            "Epoch 11/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.2047 - accuracy: 0.7017\n",
            "Epoch 12/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1671 - accuracy: 0.7078\n",
            "Epoch 13/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0981 - accuracy: 0.7250\n",
            "Epoch 14/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0457 - accuracy: 0.7364\n",
            "Epoch 15/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0052 - accuracy: 0.7439\n",
            "Epoch 16/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9775 - accuracy: 0.7489\n",
            "Epoch 17/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9966 - accuracy: 0.7446\n",
            "Epoch 18/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0741 - accuracy: 0.7229\n",
            "Epoch 19/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1250 - accuracy: 0.7042\n",
            "Epoch 20/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0933 - accuracy: 0.7148\n",
            "Epoch 21/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0673 - accuracy: 0.7240\n",
            "Epoch 22/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9894 - accuracy: 0.7394\n",
            "Epoch 23/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9022 - accuracy: 0.7664\n",
            "Epoch 24/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8907 - accuracy: 0.7691\n",
            "Epoch 25/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8371 - accuracy: 0.7782\n",
            "Epoch 26/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8322 - accuracy: 0.7850\n",
            "Epoch 27/100\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8619 - accuracy: 0.7738\n",
            "Epoch 28/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9247 - accuracy: 0.7550\n",
            "Epoch 29/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 1.1292 - accuracy: 0.7078\n",
            "Epoch 30/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1644 - accuracy: 0.6927\n",
            "Epoch 31/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0567 - accuracy: 0.7224\n",
            "Epoch 32/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9447 - accuracy: 0.7475\n",
            "Epoch 33/100\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8814 - accuracy: 0.7666\n",
            "Epoch 34/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7965 - accuracy: 0.7868\n",
            "Epoch 35/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7533 - accuracy: 0.8012\n",
            "Epoch 36/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7220 - accuracy: 0.8111\n",
            "Epoch 37/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7385 - accuracy: 0.8072\n",
            "Epoch 38/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8481 - accuracy: 0.7745\n",
            "Epoch 39/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 1.0816 - accuracy: 0.7163\n",
            "Epoch 40/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 1.2313 - accuracy: 0.6801\n",
            "Epoch 41/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.1520 - accuracy: 0.6973\n",
            "Epoch 42/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0306 - accuracy: 0.7241\n",
            "Epoch 43/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9094 - accuracy: 0.7564\n",
            "Epoch 44/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8172 - accuracy: 0.7834\n",
            "Epoch 45/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7827 - accuracy: 0.7928\n",
            "Epoch 46/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7877 - accuracy: 0.7912\n",
            "Epoch 47/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8207 - accuracy: 0.7799\n",
            "Epoch 48/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8520 - accuracy: 0.7762\n",
            "Epoch 49/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9191 - accuracy: 0.7583\n",
            "Epoch 50/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9915 - accuracy: 0.7365\n",
            "Epoch 51/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9845 - accuracy: 0.7333\n",
            "Epoch 52/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9354 - accuracy: 0.7495\n",
            "Epoch 53/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9001 - accuracy: 0.7600\n",
            "Epoch 54/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8227 - accuracy: 0.7784\n",
            "Epoch 55/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7897 - accuracy: 0.7855\n",
            "Epoch 56/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8026 - accuracy: 0.7849\n",
            "Epoch 57/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8230 - accuracy: 0.7787\n",
            "Epoch 58/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8833 - accuracy: 0.7608\n",
            "Epoch 59/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9463 - accuracy: 0.7451\n",
            "Epoch 60/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9722 - accuracy: 0.7409\n",
            "Epoch 61/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9293 - accuracy: 0.7523\n",
            "Epoch 62/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8840 - accuracy: 0.7668\n",
            "Epoch 63/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8595 - accuracy: 0.7701\n",
            "Epoch 64/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8489 - accuracy: 0.7713\n",
            "Epoch 65/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8344 - accuracy: 0.7781\n",
            "Epoch 66/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7965 - accuracy: 0.7878\n",
            "Epoch 67/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7820 - accuracy: 0.7868\n",
            "Epoch 68/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7743 - accuracy: 0.7925\n",
            "Epoch 69/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8927 - accuracy: 0.7630\n",
            "Epoch 70/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0120 - accuracy: 0.7364\n",
            "Epoch 71/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0473 - accuracy: 0.7286\n",
            "Epoch 72/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 1.0244 - accuracy: 0.7315\n",
            "Epoch 73/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.9415 - accuracy: 0.7539\n",
            "Epoch 74/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8666 - accuracy: 0.7693\n",
            "Epoch 75/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8143 - accuracy: 0.7825\n",
            "Epoch 76/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7764 - accuracy: 0.7920\n",
            "Epoch 77/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7772 - accuracy: 0.7894\n",
            "Epoch 78/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8118 - accuracy: 0.7835\n",
            "Epoch 79/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8295 - accuracy: 0.7787\n",
            "Epoch 80/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8224 - accuracy: 0.7774\n",
            "Epoch 81/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8136 - accuracy: 0.7799\n",
            "Epoch 82/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8143 - accuracy: 0.7796\n",
            "Epoch 83/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8404 - accuracy: 0.7772\n",
            "Epoch 84/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8164 - accuracy: 0.7796\n",
            "Epoch 85/100\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7949 - accuracy: 0.7847\n",
            "Epoch 86/100\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8248 - accuracy: 0.7780\n",
            "Epoch 87/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8209 - accuracy: 0.7794\n",
            "Epoch 88/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8041 - accuracy: 0.7855\n",
            "Epoch 89/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8123 - accuracy: 0.7839\n",
            "Epoch 90/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8426 - accuracy: 0.7737\n",
            "Epoch 91/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8400 - accuracy: 0.7755\n",
            "Epoch 92/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8320 - accuracy: 0.7793\n",
            "Epoch 93/100\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.8216 - accuracy: 0.7757\n",
            "Epoch 94/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7771 - accuracy: 0.7917\n",
            "Epoch 95/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.7711 - accuracy: 0.7907\n",
            "Epoch 96/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.7738 - accuracy: 0.7925\n",
            "Epoch 97/100\n",
            "377/377 [==============================] - 3s 9ms/step - loss: 0.7665 - accuracy: 0.7935\n",
            "Epoch 98/100\n",
            "377/377 [==============================] - 4s 10ms/step - loss: 0.8137 - accuracy: 0.7794\n",
            "Epoch 99/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.8652 - accuracy: 0.7678\n",
            "Epoch 100/100\n",
            "377/377 [==============================] - 4s 9ms/step - loss: 0.9083 - accuracy: 0.7594\n",
            "<tensorflow.python.keras.engine.sequential.Sequential object at 0x7fda3d267b00>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt9gRwfETz2G"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHqytec3T2Ce",
        "outputId": "65674c3c-9903-4b7d-c148-22623c4a6566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjkarXrT2sN",
        "outputId": "1ea1cbed-9b7d-4638-a47f-aea5db01495d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seed_texts = \"I've got a bad feeling about this\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([seed_texts])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = model.predict_classes(token_list, verbose=0)\n",
        "    output_word = \"\"\n",
        "\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    \n",
        "    seed_texts += \" \" + output_word\n",
        "\n",
        "print(seed_texts)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I've got a bad feeling about this narrow street as a bright as die the cliffs of doneen the way can put me glass in his honor of gone the covers buttoned down by the town of athy gone gone there for me gone by the month of gone the love forever by the hand the pipers tune with the lark soar high gone me one more gone and gone him on the gone by the month of old times more she said she see me gone him off wid the rest for the gone the gone gone the girls alone oh haste gone the fairest flower\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q11tGTbgUqbT"
      },
      "source": [
        "### Part III \n",
        "\n",
        "### Character-based prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvNfebGKUaoJ"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr0tRaPXWZMe",
        "outputId": "15718575-1872-4607-b3f2-81e21b6d09f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# download the dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ZlbLTVWa0V",
        "outputId": "7067e665-cc2a-433c-c9ad-21330887311b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# read the data\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmQwqSrTW4NU",
        "outputId": "81de99e4-8beb-43f1-ee8d-a13161235bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "# first 250 characters in text\n",
        "print(text[:250])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvn2QJFyW7gE",
        "outputId": "0f627296-3eea-4cc3-e9c7-65ab89a14d4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# unique chars in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUoOaZgNa0af"
      },
      "source": [
        "Vectorize the text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmKEt7nYXFJ_"
      },
      "source": [
        "# create a mapping from unique characters to indices\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[char] for char in text])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htnmLoIObGOx",
        "outputId": "7aea878e-a362-42bc-d5c5-7a3b8defd8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6E5t3DsbIcR",
        "outputId": "adfe3350-7aac-4a80-fc11-0ad7218928db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IrkhU-2bVFg"
      },
      "source": [
        "create training examples and targets\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIwUcaI9bLyM"
      },
      "source": [
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1) # For example, say seq_length is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\"."
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeA3yad5bphO",
        "outputId": "2b55f467-062f-48fe-c49f-9148d5186097",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UX5bl5wbzWu",
        "outputId": "ab3b38a4-469d-4248-b3ed-53d09393026e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "# batch method easily converts the individual chars to sequences of desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpFIgu25cPC-"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFGK-E9icFlL"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ2YX7CyceFc",
        "outputId": "7ca73ae9-18e6-4df8-f4eb-7579970e8ef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr0vBFAeclT8",
        "outputId": "b601a8c4-18af-47a2-9efa-364db894c24e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 18 ('F')\n",
            "  expected output: 47 ('i')\n",
            "Step    1\n",
            "  input: 47 ('i')\n",
            "  expected output: 56 ('r')\n",
            "Step    2\n",
            "  input: 56 ('r')\n",
            "  expected output: 57 ('s')\n",
            "Step    3\n",
            "  input: 57 ('s')\n",
            "  expected output: 58 ('t')\n",
            "Step    4\n",
            "  input: 58 ('t')\n",
            "  expected output: 1 (' ')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw_g6ztqc19l"
      },
      "source": [
        "Create training batches\n",
        "\n",
        "use `tf.data` to split the text into manageable sequences and shuffle the data and pack it into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP7pgtqwc06I",
        "outputId": "a1ce0981-6b93-4cb0-ad43-65d7b801830c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "# buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r3IvF6odQH8"
      },
      "source": [
        "### Build the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f5ligwudO9w"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgAD0UMcdUvn"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmdYr9K6dXhF"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv_zP6Dsdk45"
      },
      "source": [
        "For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymlBgC_ed-LT"
      },
      "source": [
        "Run the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B25xm0kcdgKO",
        "outputId": "af4f24f5-fb83-4dc7-fb1e-4c95d8620585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#check the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WXaXNjWeGmv",
        "outputId": "568be7ee-7deb-411e-bbed-8f70cabf569a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDZI3lBPehcu"
      },
      "source": [
        "To get actual predictions from the model we need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqvqgyGTeI9i"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbsihZlIeMdk",
        "outputId": "b1222dc2-b0f0-43ec-d7c3-c37f3c1b1555",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18,  5, 60, 18, 41, 32, 49, 14, 40, 49,  3, 63, 63, 45, 54, 42, 21,\n",
              "       56,  1, 19, 36, 37, 50,  1, 38, 34, 20, 28, 56,  5, 60, 34, 24, 43,\n",
              "       23, 55, 28,  7, 13,  1, 23, 26,  4, 25, 47, 35, 49, 56, 26, 34, 33,\n",
              "       14, 51, 44, 11, 48, 57, 36, 12, 61, 52, 55, 34, 50, 12, 33, 17, 46,\n",
              "       45, 63, 43, 19, 15,  3, 46, 26, 45, 44, 25, 46, 44, 11, 10,  4, 47,\n",
              "       34,  2, 17, 30, 45, 62, 56, 10, 32,  7, 27, 16, 39, 18, 64])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq257oJqeZJA",
        "outputId": "0af19b31-2b24-4fad-9e4a-d8055d2766e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'tery\\n\\nPOMPEY:\\nPainting, sir, I have heard say, is a mystery; and\\nyour whores, sir, being members of '\n",
            "\n",
            "Next Char Predictions: \n",
            " \"F'vFcTkBbk$yygpdIr GXYl ZVHPr'vVLeKqP-A KN&MiWkrNVUBmf;jsX?wnqVl?UEhgyeGC$hNgfMhf;:&iV!ERgxr:T-ODaFz\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmxi_oZredCU"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoSbUAV-eb5h",
        "outputId": "ffcf737d-759c-49be-dd14-c276863aea90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.1735573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvBiFLDeoDB"
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpBgihZReuHb"
      },
      "source": [
        "# Dir where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "\n",
        "checkpoint_prefix =  os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath = checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGhoNgXWfNrI"
      },
      "source": [
        "Execute the training "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYNHRRZRfL08",
        "outputId": "ddf058db-bfae-47ed-908f-98a721abad1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(dataset, \n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[checkpoint_callback])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 2.6909\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.9642\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 10s 58ms/step - loss: 1.6937\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 10s 57ms/step - loss: 1.5445\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.4564\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3964\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3505\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.3134\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2778\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 10s 56ms/step - loss: 1.2449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0uQ-dCJfX2-"
      },
      "source": [
        "Generate the Text\n",
        "\n",
        "- Restore the latest checkpoint\n",
        "  - to keep the prediction step simple, use batch size of 1\n",
        "  - the model only accepts a fixed batch size once built\n",
        "  - to run the model with different batch size, we need to rebuild the model and restore the weights from the checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwBwEJNVfVKg",
        "outputId": "81ee8e94-88c1-4162-f40f-b6b1a2dc6a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5oGqYRNftzz"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWKKxw3Hf1Tn",
        "outputId": "a4a48e3e-ebde-40cc-fbd7-0313065f1a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (1, None, 256)            16640     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (1, None, 65)             66625     \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jXYhao6f7Ub"
      },
      "source": [
        "### The prediction loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09wPaDPSf_Hp"
      },
      "source": [
        "### The prediction loop\n",
        "\n",
        "The following code block generates the text:\n",
        "\n",
        "* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
        "\n",
        "* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one character. After predicting the next character, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "\n",
        "![To generate text the model's output is fed back to the input](images/text_generation_sampling.png)\n",
        "\n",
        "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuwZBX5Ogfd"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # We pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbPETAfrgcMA",
        "outputId": "d776437d-ae53-4a2d-8c8b-c96143d7a39d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 625
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: be put notice;\n",
            "If I disdain this sit on you and your kind, kiss be remember'd.\n",
            "\n",
            "ARCHBISHOP OF YORK:\n",
            "At the dark crosps and Trbale of Hereford!\n",
            "What state, hasting your wereless conference;\n",
            "Serving make he seeming stons himself the winds here in mine delitions?\n",
            "He will descry him.\n",
            "\n",
            "LEONTES:\n",
            "You't are the assembly to your souls!--\n",
            "Wife to't is leap gone, sir, to say thy change-\n",
            "Her Past, ha! besides the any numbly of thee.\n",
            "\n",
            "CORIOLANUS:\n",
            "I am a wonder of what's toverened;\n",
            "Werm in thy speech her not, for I shall do forget\n",
            "Lest in the regal easily mercy office,\n",
            "You brother to tell you.\n",
            "\n",
            "CORIOLANUS:\n",
            "O my brower is!--if you were stone; 'tis touch'd to murchard sea.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Nathre, condition! What may not delave\n",
            "the cail that doth command in groans; and yet\n",
            "mall heaven consarren and wattle white officer:\n",
            "Come, we will stand up his awmithat so betimes,\n",
            "You'll make a declorbind vowils to hold,\n",
            "Besome with Telivian, two\n",
            "if Itlambol-hate?\n",
            "\n",
            "CAMILLO:\n",
            "Well, assistatcle: that afflicts it is\n",
            "g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBPqso8dgfvB"
      },
      "source": [
        "The easiest thing you can do to improve the results it to train it for longer (try EPOCHS=30).\n",
        "\n",
        "You can also experiment with a different start string, or try adding another RNN layer to improve the model's accuracy, or adjusting the temperature parameter to generate more or less random predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWj84poignXf"
      },
      "source": [
        "## Customized Training\n",
        "\n",
        "`tf.GradientTape` to track the gradients\n",
        "\n",
        "procedure:\n",
        "  - First, reset the RNN state, by calling `tf.keras.Model.reset_states`\n",
        "  - Next, iterate over the dataset (batch by batch) and calculate the predictions associated with each\n",
        "  - Open a `tf.GradientTape`, and calculate the predictions and loss in that context\n",
        "  - Calculate the gradients of the loss with respect to the model variables using the `tf.GradientTape.grads` method\n",
        "  - Finally, take a step downwards by using the optimizer's `tf.train.Optimizer.apply_gradients` method\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5oKCIlPhL-M",
        "outputId": "1e7f1bfc-16e5-40ce-aa53-31d3f66646ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = len(vocab),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.embeddings\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_fG9lRXhNiS"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHuGwYwBheXH"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inp)\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.keras.losses.sparse_categorical_crossentropy(\n",
        "                target, predictions, from_logits=True))\n",
        "    \n",
        "    grads = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeA9RYEqh5Ap",
        "outputId": "7c54bcaa-4b7c-4496-eee3-d985c1553c8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # resetting the hidden state at the start of every epoch\n",
        "    model.reset_states()\n",
        "\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "        template = 'Epoch {} Batch {} Loss {}'\n",
        "        print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss 2.1627\n",
            "Time taken for 1 epoch 10.830343246459961 sec\n",
            "\n",
            "Epoch 2 Loss 1.7915\n",
            "Time taken for 1 epoch 9.942629337310791 sec\n",
            "\n",
            "Epoch 3 Loss 1.6120\n",
            "Time taken for 1 epoch 10.108837127685547 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}